{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fig_utils import load_hooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 4\n",
      "GPU 0: NVIDIA L40S\n",
      "  - Total Memory: 44.32 GB\n",
      "  - Allocated Memory: 0.00 GB\n",
      "  - Reserved Memory: 0.00 GB\n",
      "  - Free Memory: 44.32 GB\n",
      "GPU 1: NVIDIA L40S\n",
      "  - Total Memory: 44.32 GB\n",
      "  - Allocated Memory: 0.00 GB\n",
      "  - Reserved Memory: 0.00 GB\n",
      "  - Free Memory: 44.32 GB\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "  - Total Memory: 79.15 GB\n",
      "  - Allocated Memory: 0.00 GB\n",
      "  - Reserved Memory: 0.00 GB\n",
      "  - Free Memory: 79.15 GB\n",
      "GPU 3: NVIDIA A100 80GB PCIe\n",
      "  - Total Memory: 79.15 GB\n",
      "  - Allocated Memory: 0.00 GB\n",
      "  - Reserved Memory: 0.00 GB\n",
      "  - Free Memory: 79.15 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    gpu_properties = torch.cuda.get_device_properties(i)\n",
    "    total_memory = gpu_properties.total_memory / 1024**3  # Convert bytes to GB\n",
    "    allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "    reserved_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "    free_memory = total_memory - allocated_memory - reserved_memory\n",
    "\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  - Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"  - Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"  - Reserved Memory: {reserved_memory:.2f} GB\")\n",
    "    print(f\"  - Free Memory: {free_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0) # Set all operations to run on cuda:1\n",
    "device = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()  # Clears unused cached memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kebl6672/dpo-toxic-neuron/toxicity/fig_utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _weights = torch.load(weights_path, map_location=torch.device(\"cuda\"))[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = '/data/kebl6672/dpo-toxic-neuron/checkpoints'\n",
    "# Load model and move it to the correct device\n",
    "dpo_model = load_hooked(\"gpt2-medium\", os.path.join(ROOT_DIR, \"dpo.pt\"))\n",
    "\n",
    "# Move the model explicitly to the correct device\n",
    "dpo_model.to(device)\n",
    "# # Force loading the weights to the correct device\n",
    "# dpo_model.load_and_process_state_dict(\n",
    "#     torch.load(os.path.join(ROOT_DIR, \"dpo.pt\"), map_location=\"cuda:2\")\n",
    "# )\n",
    "\n",
    "  # Move DPO model to device\n",
    "# print(f\"Model is now on: {next(dpo_model.parameters()).device}\")\n",
    "\n",
    "# torch.load(os.path.join(ROOT_DIR, \"dpo.pt\"), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2 = HookedTransformer.from_pretrained(\"gpt2-medium\")\n",
    "gpt2.tokenizer.padding_side = \"left\"\n",
    "gpt2.tokenizer.pad_token_id = gpt2.tokenizer.eos_token_id\n",
    "gpt2.to(device)  # Move GPT-2 to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3410612/4142172117.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  toxic_vector = torch.load(os.path.join(ROOT_DIR, \"probe.pt\")).to(device)  # Move toxic probe vector to device\n"
     ]
    }
   ],
   "source": [
    "toxic_vector = torch.load(os.path.join(ROOT_DIR, \"probe.pt\")).to(device)  # Move toxic probe vector to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "DATA_DIR = '/data/kebl6672/dpo-toxic-neuron/data/intervene_data'\n",
    "\n",
    "with open(\n",
    "    os.path.join(DATA_DIR, \"challenge_prompts_prefix_mmm.jsonl\"), \"r\"\n",
    "    # os.path.join(ROOT_DIR, \"challenge_prompts.jsonl\"), \"r\"\n",
    ") as file_p:\n",
    "    data = file_p.readlines()\n",
    "\n",
    "prompts = [json.loads(x.strip())[\"prompt\"] for x in data]\n",
    "\n",
    "tokenized_prompts = dpo_model.to_tokens(prompts, prepend_bos=True).cuda()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input and output projection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_toxic_projection_reduction(model, tokenized_prompts, toxic_vector, batch_size=64):\n",
    "\n",
    "    # Dictionary to store hook data\n",
    "    mlp_inputs = {}\n",
    "\n",
    "    # Function to create a hook function for a specific layer\n",
    "    def create_store_mlp_input_hook(layer_idx):\n",
    "        def store_mlp_input_hook(module, inputs, outputs):\n",
    "            mlp_inputs[layer_idx] = inputs[0].detach()\n",
    "        return store_mlp_input_hook\n",
    "\n",
    "    # Add the hooks to each MLP layer in the model\n",
    "    for layer_idx, layer in enumerate(model.blocks):\n",
    "        layer.mlp.register_forward_hook(create_store_mlp_input_hook(layer_idx))\n",
    "\n",
    "    # Initialize dictionaries to store projections for all layers\n",
    "    gpt2_input_projections = defaultdict(list)\n",
    "    gpt2_output_projections = defaultdict(list)\n",
    "    gpt2_projection_reductions = defaultdict(list)\n",
    "\n",
    "    sample_size = tokenized_prompts.size(0)\n",
    "\n",
    "    print(\"Computing MLP inputs and output projections...\")\n",
    "    for idx in tqdm(range(0, sample_size, batch_size)):\n",
    "        batch = tokenized_prompts[idx : idx + batch_size, :]\n",
    "\n",
    "        # Clear the mlp_inputs dictionary before running the batch\n",
    "        mlp_inputs.clear()\n",
    "\n",
    "        for timestep in range(20): # generate 20 tokens\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(batch)\n",
    "\n",
    "            sampled = model.unembed(cache[\"ln_final.hook_normalized\"]).argmax(-1)[:, -1]\n",
    "\n",
    "            for layer_idx in range(len(model.blocks)):\n",
    "                # Retrieve the input to the MLP stored by the hook\n",
    "                x_token = mlp_inputs[layer_idx][:, -1, :] # at the last token position\n",
    "\n",
    "                # Get the output after the MLP has been applied (before adding back to the residual stream)\n",
    "                y_token = cache[f\"blocks.{layer_idx}.hook_mlp_out\"][:, -1, :] # [batch_size, d_model]\n",
    "\n",
    "                # print(torch.norm(x_token), torch.norm(y_token))\n",
    "                \n",
    "                # Compute the projection onto the toxic vector for the MLP input and output\n",
    "                input_projection = torch.matmul(x_token, toxic_vector) / torch.norm(toxic_vector) # [batch_size]\n",
    "                output_projection = torch.matmul(y_token, toxic_vector) / torch.norm(toxic_vector) # [batch_size]\n",
    "\n",
    "                # Store the mean projection values for this batch and layer\n",
    "                gpt2_input_projections[layer_idx].extend(input_projection.tolist())\n",
    "                gpt2_output_projections[layer_idx].extend(output_projection.tolist())\n",
    "\n",
    "                # Compute the projection reduction (how much the MLP changes the toxic direction)\n",
    "                projection_reduction = input_projection - output_projection\n",
    "                gpt2_projection_reductions[layer_idx].extend(projection_reduction.tolist())\n",
    "\n",
    "            batch = torch.concat([batch, sampled.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Compute final average projections across all batches and tokens for each layer\n",
    "    avg_input_projections = {layer_idx: np.mean(projections) for layer_idx, projections in gpt2_input_projections.items()}\n",
    "    avg_output_projections = {layer_idx: np.mean(projections) for layer_idx, projections in gpt2_output_projections.items()}\n",
    "    avg_projection_reductions = {layer_idx: np.mean(projections) for layer_idx, projections in gpt2_projection_reductions.items()}\n",
    "\n",
    "    return avg_input_projections, avg_output_projections, avg_projection_reductions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_input_projections, avg_output_projections, avg_projection_reductions = compute_layer_toxic_projection_reduction(gpt2, tokenized_prompts, toxic_vector)\n",
    "avg_input_projections_dpo, avg_output_projections_dpo, avg_projection_reductions_dpo = compute_layer_toxic_projection_reduction(dpo_model, tokenized_prompts, toxic_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ablate top toxic value vectors and plot output projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_top_value_vectors(model, toxic_vector, topk_sorted_score, scale_factor=0):\n",
    "    \"\"\"\n",
    "    Scales the value vectors of the MLPs based on the cosine similarity of the value vectors to a given toxic vector.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model being modified.\n",
    "        toxic_vector (torch.Tensor): The vector to compare against for scaling.\n",
    "        topk_sorted_score (int): The number of top vectors to scale.\n",
    "        scale_factor (float): The factor by which to scale the selected vectors.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Module: The modified model with scaled value vectors.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Iterate through each layer of the model\n",
    "    for layer_idx in range(len(model.blocks)):\n",
    "        # Target the value vectors in the MLP\n",
    "        value_vectors = model.blocks[layer_idx].mlp.W_out\n",
    "        \n",
    "        # Compute cosine similarities between the value vectors and the toxic vector\n",
    "        cos_sims = F.cosine_similarity(value_vectors, toxic_vector.unsqueeze(0), dim=1)\n",
    "        \n",
    "        # Get the top k most similar value vectors\n",
    "        _topk = cos_sims.topk(k=100)\n",
    "        _values = [x.item() for x in _topk.values]\n",
    "        _idxs = [x.item() for x in _topk.indices]\n",
    "        topk = list(zip(_values, _idxs, [layer_idx] * _topk.indices.shape[0]))\n",
    "        scores.extend(topk)\n",
    "\n",
    "    # Sort the scores in descending order based on cosine similarity\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Select the top `topk_sorted_score` value vectors and scale them\n",
    "    with torch.no_grad():\n",
    "        for score in sorted_scores[:topk_sorted_score]:\n",
    "            layer_idx, neuron_idx = score[2], score[1]\n",
    "            print(f\"Scaling vector at layer {layer_idx}, index {neuron_idx} by {scale_factor}\")\n",
    "            model.blocks[layer_idx].mlp.W_out[neuron_idx, :] *= scale_factor\n",
    "\n",
    "    # Return the modified model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_top_value_vectors_with_positive_activations(model, toxic_positive_acts_index_csv_path, topk_sorted_score, scale_factor=0):\n",
    "    \"\"\"\n",
    "    Scales the value vectors with positive activations before DPO based on the ranks of their cosine similarity with the toxic probe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the sorted scores from the CSV\n",
    "    sorted_scores_df = pd.read_csv(toxic_positive_acts_index_csv_path)\n",
    "    \n",
    "    # Select the top `topk_sorted_score` layer and neuron indices from the CSV\n",
    "    top_layer_neuron_indices = sorted_scores_df.head(topk_sorted_score)\n",
    "    \n",
    "    # Scale the selected value vectors\n",
    "    with torch.no_grad():\n",
    "        for _, row in top_layer_neuron_indices.iterrows():\n",
    "            layer_idx = row['layer_idx']\n",
    "            neuron_idx = row['neuron_idx']\n",
    "            print(f\"Scaling vector at layer {layer_idx}, index {neuron_idx} by {scale_factor}\")\n",
    "            model.blocks[layer_idx].mlp.W_out[neuron_idx, :] *= scale_factor\n",
    "\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable top 128 toxic value vectors\n",
    "new_gpt2 = scale_top_value_vectors(gpt2, toxic_vector, 128, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable top 128 toxic value vectors with positive activations\n",
    "# new_gpt2 = scale_top_value_vectors_with_positive_activations(gpt2, './toxic_positive_acts_idxs.csv', 128, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_gpt2.blocks[19].mlp.W_out[770, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_avg_input_projections, new_avg_output_projections, new_avg_projection_reductions = compute_layer_toxic_projection_reduction(new_gpt2, tokenized_prompts, toxic_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_layer_projections(layer_input_projections1, layer_output_projections1,\n",
    "                           layer_input_projections2, layer_output_projections2):\n",
    "    # Ensure all projection lists are sorted by layer index\n",
    "    layers1 = sorted(layer_input_projections1.keys())\n",
    "    layers2 = sorted(layer_input_projections2.keys())\n",
    "    \n",
    "    # Convert dictionaries to lists based on the sorted layers\n",
    "    layer_input_projections1 = [layer_input_projections1[layer] for layer in layers1]\n",
    "    layer_output_projections1 = [layer_output_projections1[layer] for layer in layers1]\n",
    "    layer_input_projections2 = [layer_input_projections2[layer] for layer in layers2]\n",
    "    layer_output_projections2 = [layer_output_projections2[layer] for layer in layers2]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(layers1, layer_input_projections1, label='Pre-DPO Input Projection', marker='o', linestyle='-', color='blue')\n",
    "    plt.plot(layers1, layer_output_projections1, label='Pre-DPO Output Projection', marker='x', linestyle='--', color='orange')\n",
    "    \n",
    "    plt.plot(layers2, layer_input_projections2, label='DPO Input Projection', marker='o', linestyle='-', color='green')\n",
    "    plt.plot(layers2, layer_output_projections2, label='DPO Output Projection', marker='x', linestyle='--', color='red')\n",
    "\n",
    "    plt.title('Input and Output Projections Across Layers')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Projection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_layer_projections(layer_input_projections1, layer_output_projections1,\n",
    "                           layer_input_projections2, layer_output_projections2,\n",
    "                           layer_input_projections3, layer_output_projections3):\n",
    "    # Ensure all projection lists are sorted by layer index\n",
    "    layers1 = sorted(layer_input_projections1.keys())\n",
    "    layers2 = sorted(layer_input_projections2.keys())\n",
    "    layers3 = sorted(layer_input_projections3.keys())\n",
    "    \n",
    "    # Convert dictionaries to lists based on the sorted layers\n",
    "    layer_input_projections1 = [layer_input_projections1[layer] for layer in layers1]\n",
    "    layer_output_projections1 = [layer_output_projections1[layer] for layer in layers1]\n",
    "    layer_input_projections2 = [layer_input_projections2[layer] for layer in layers2]\n",
    "    layer_output_projections2 = [layer_output_projections2[layer] for layer in layers2]\n",
    "    layer_input_projections3 = [layer_input_projections3[layer] for layer in layers3]\n",
    "    layer_output_projections3 = [layer_output_projections3[layer] for layer in layers3]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot first set of input and output projections\n",
    "    plt.plot(layers1, layer_input_projections1, label='Pre-DPO Input Projection', marker='o', linestyle='-', color='blue')\n",
    "    plt.plot(layers1, layer_output_projections1, label='Pre-DPO Output Projection', marker='x', linestyle='--', color='orange')\n",
    "    \n",
    "    # Plot second set of input and output projections\n",
    "    plt.plot(layers2, layer_input_projections2, label='Disable Value Vector Input Projection', marker='o', linestyle='-', color='green')\n",
    "    plt.plot(layers2, layer_output_projections2, label='Disable Value Vector Output Projection', marker='x', linestyle='--', color='red')\n",
    "    \n",
    "    # Plot third set of input and output projections\n",
    "    plt.plot(layers3, layer_input_projections3, label='DPO Input Projection', marker='o', linestyle='-', color='purple')\n",
    "    plt.plot(layers3, layer_output_projections3, label='DPO Output Projection', marker='x', linestyle='--', color='brown')\n",
    "\n",
    "    plt.title('Input and Output Projections Across Layers')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Projection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_layer_projections_output(layer_output_projections1,\n",
    "                                        layer_output_projections2,\n",
    "                                        layer_output_projections3):\n",
    "    # Ensure all projection lists are sorted by layer index\n",
    "    layers1 = sorted(layer_output_projections1.keys())\n",
    "    layers2 = sorted(layer_output_projections2.keys())\n",
    "    layers3 = sorted(layer_output_projections3.keys())\n",
    "    \n",
    "    # Convert dictionaries to lists based on the sorted layers\n",
    "    layer_output_projections1 = [layer_output_projections1[layer] for layer in layers1]\n",
    "    layer_output_projections2 = [layer_output_projections2[layer] for layer in layers2]\n",
    "    layer_output_projections3 = [layer_output_projections3[layer] for layer in layers3]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot first set of output projections with increased line width\n",
    "    plt.plot(layers1, layer_output_projections1, label='Before DPO', marker='x', linestyle='--', color='red', linewidth=2.5)\n",
    "    \n",
    "    # Plot second set of output projections with increased line width\n",
    "    plt.plot(layers2, layer_output_projections2, label='Ablate 128 toxic neurons', marker='x', linestyle='--', color='orange', linewidth=2.5)\n",
    "    \n",
    "    # Plot third set of output projections with increased line width\n",
    "    plt.plot(layers3, layer_output_projections3, label='After DPO', marker='x', linestyle='--', color='green', linewidth=2.5)\n",
    "\n",
    "    # plt.title('Output projection per layer')\n",
    "    plt.xlabel('MLP layer index', fontsize=20)\n",
    "    plt.ylabel('Output projection per layer', fontsize=20)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    # plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_layer_projections_output(layer_output_projections1,\n",
    "                                 layer_output_projections2):\n",
    "    # Ensure all projection lists are sorted by layer index\n",
    "    layers1 = sorted(layer_output_projections1.keys())\n",
    "    layers2 = sorted(layer_output_projections2.keys())\n",
    "    \n",
    "    # Convert dictionaries to lists based on the sorted layers\n",
    "    layer_output_projections1 = [layer_output_projections1[layer] for layer in layers1]\n",
    "    layer_output_projections2 = [layer_output_projections2[layer] for layer in layers2]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot first set of output projections\n",
    "    plt.plot(layers1, layer_output_projections1, label='Pre-DPO Output Projection', marker='x', linestyle='--', color='red')\n",
    "    \n",
    "    # Plot second set of output projections\n",
    "    plt.plot(layers2, layer_output_projections2, label='DPO Output Projection', marker='x', linestyle='--', color='green')\n",
    "\n",
    "    plt.title('Output Projections Across Layers')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Projection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Projection after disabling toxic value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "plot_two_layer_projections_output(avg_output_projections, \n",
    "                      avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "# Disable top 128 value vectors\n",
    "plot_three_layer_projections_output(avg_output_projections, \n",
    "                      new_avg_output_projections,\n",
    "                      avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "# Disable top 200 value vectors with positive activations\n",
    "plot_three_layer_projections_output(avg_output_projections, \n",
    "                      new_avg_output_projections,\n",
    "                      avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "# Disable top 1000 value vectors with positive activations (legend is wrong)\n",
    "plot_three_layer_projections_output(avg_output_projections, \n",
    "                      new_avg_output_projections,\n",
    "                      avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "# Disable top 2000 value vectors with positive activations (legend is wrong)\n",
    "plot_three_layer_projections_output(avg_output_projections, \n",
    "                      new_avg_output_projections,\n",
    "                      avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged over 20 tokens\n",
    "# Disable top 200 value vectors\n",
    "plot_three_layer_projections(avg_input_projections, avg_output_projections, \n",
    "                       new_avg_input_projections, new_avg_output_projections,\n",
    "                       avg_input_projections_dpo, avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the last token position only (for generating the next token)\n",
    "# Disable top 200 value vectors\n",
    "plot_layer_projections(avg_input_projections, avg_output_projections, \n",
    "                       avg_input_projections_dpo, avg_output_projections_dpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuron toxicity projection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neuron_toxic_projection(model, tokenized_prompts, toxic_vector, batch_size=64):\n",
    "    # Initialize dictionaries to store projections and activations for all layers\n",
    "    gpt2_neuron_projections = defaultdict(list)\n",
    "\n",
    "    sample_size = tokenized_prompts.size(0)\n",
    "\n",
    "    print(\"Computing MLP neuron projections...\")\n",
    "    \n",
    "    device = next(model.parameters()).device  # Get the model's device\n",
    "\n",
    "    for idx in tqdm(range(0, sample_size, batch_size)):\n",
    "        batch = tokenized_prompts[idx : idx + batch_size, :].to(device)\n",
    "\n",
    "        for timestep in range(20):  # generate 20 tokens\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(batch)\n",
    "\n",
    "            # Ensure cache tensors are moved to the correct device\n",
    "            cache = {k: v.to(device).detach().clone() for k, v in cache.items()}\n",
    "\n",
    "            sampled = model.unembed(cache[\"ln_final.hook_normalized\"]).argmax(-1).detach().to(device)[:, -1]\n",
    "\n",
    "            for layer_idx in range(len(model.blocks)):\n",
    "                neuron_acts = cache[f\"blocks.{layer_idx}.mlp.hook_post\"][:, -1, :].to(device)\n",
    "                value_vectors = model.blocks[layer_idx].mlp.W_out.to(device)\n",
    "\n",
    "                neuron_outputs = neuron_acts.unsqueeze(-1) * value_vectors\n",
    "                neuron_projections = torch.matmul(neuron_outputs, toxic_vector.to(device)) / torch.norm(toxic_vector.to(device))\n",
    "\n",
    "                for neuron_idx in range(neuron_projections.size(1)):\n",
    "                    gpt2_neuron_projections[(layer_idx, neuron_idx)].extend(neuron_projections[:, neuron_idx].tolist())\n",
    "\n",
    "            batch = torch.concat([batch, sampled.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Compute final average neuron projections and average activations across all batches and tokens \n",
    "    avg_neuron_projections = {\n",
    "        (layer_idx, neuron_idx): np.mean(projections)\n",
    "        for (layer_idx, neuron_idx), projections in gpt2_neuron_projections.items()\n",
    "    }\n",
    "\n",
    "    return avg_neuron_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check neuron projection calculation correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_projection_sums(model, tokenized_prompts, toxic_vector, batch_size=64):\n",
    "    # Initialize dictionaries to store the sum and count of projections for all layers\n",
    "    gpt2_neuron_projections_sum = defaultdict(float)\n",
    "    gpt2_neuron_projections_count = defaultdict(int)\n",
    "\n",
    "    sample_size = tokenized_prompts.size(0)\n",
    "\n",
    "    print(\"Computing MLP neuron projections and averaging them for each layer...\")\n",
    "    for idx in tqdm(range(0, sample_size, batch_size)):\n",
    "        batch = tokenized_prompts[idx : idx + batch_size, :]\n",
    "\n",
    "        for timestep in range(20):  # generate 20 tokens\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(batch)\n",
    "\n",
    "            sampled = model.unembed(cache[\"ln_final.hook_normalized\"]).argmax(-1)[:, -1]\n",
    "\n",
    "            for layer_idx in range(len(model.blocks)):\n",
    "                # Extract neuron activations before applying the output weights\n",
    "                neuron_acts = cache[f\"blocks.{layer_idx}.mlp.hook_post\"][:, -1, :]  # [batch_size, d_mlp]\n",
    "                value_vectors = model.blocks[layer_idx].mlp.W_out  # [d_mlp, d_model]\n",
    "\n",
    "                # Compute the neuron outputs by multiplying neuron_acts with the corresponding value vector\n",
    "                neuron_outputs = neuron_acts.unsqueeze(-1) * value_vectors  # [batch_size, d_mlp, d_model]\n",
    "\n",
    "                # Compute the projection onto the toxic vector for each neuron output\n",
    "                neuron_projections = torch.matmul(neuron_outputs, toxic_vector) / torch.norm(toxic_vector)  # [batch_size, d_mlp]\n",
    "                \n",
    "                # Sum the projections across all neurons in the layer for this batch\n",
    "                layer_projection_sum = torch.sum(neuron_projections, dim=1)  # [batch_size]\n",
    "\n",
    "                # Accumulate the sum of projections for the layer and keep track of the count\n",
    "                gpt2_neuron_projections_sum[layer_idx] += torch.sum(layer_projection_sum).item()\n",
    "                gpt2_neuron_projections_count[layer_idx] += batch_size\n",
    "\n",
    "            # Update the batch with the newly sampled tokens\n",
    "            batch = torch.concat([batch, sampled.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Calculate the final averaged projection for each layer and add the bias projection\n",
    "    final_layer_projections = {}\n",
    "    for layer_idx in range(len(model.blocks)):\n",
    "        # Average the accumulated projections\n",
    "        avg_projection = gpt2_neuron_projections_sum[layer_idx] / gpt2_neuron_projections_count[layer_idx]\n",
    "        \n",
    "        # Add the bias projection\n",
    "        bias = model.blocks[layer_idx].mlp.b_out  # [d_model]\n",
    "        bias_projection = torch.dot(bias, toxic_vector) / torch.norm(toxic_vector)  # scalar\n",
    "\n",
    "        # Final projection for this layer\n",
    "        final_layer_projections[layer_idx] = avg_projection + bias_projection.item()\n",
    "\n",
    "    return final_layer_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_projections(layer_projections):\n",
    "    layers = list(layer_projections.keys())\n",
    "    projections = list(layer_projections.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(layers, projections, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Final Projection Sum')\n",
    "    plt.title('Final Layer Projections for Each Layer')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_projections = compute_layer_projection_sums(gpt2, tokenized_prompts, toxic_vector)\n",
    "plot_layer_projections(final_layer_projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute neuron toxicity projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv file\n",
    "def save_neuron_projections_to_csv(avg_neuron_projections, filename):\n",
    "    # Convert the dictionary to a list of tuples (layer_idx, neuron_idx, projection_value)\n",
    "    data = [\n",
    "        {\"layer_idx\": layer_idx, \"neuron_idx\": neuron_idx, \"projection_value\": projection}\n",
    "        for (layer_idx, neuron_idx), projection in avg_neuron_projections.items()\n",
    "    ]\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Neuron projections saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MLP neuron projections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 44.32 GiB of which 8.50 MiB is free. Process 3078353 has 6.58 GiB memory in use. Including non-PyTorch memory, this process has 37.72 GiB memory in use. Of the allocated memory 35.06 GiB is allocated by PyTorch, and 2.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Compute the neuron data and average projections\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m avg_neuron_projections \u001b[39m=\u001b[39m compute_neuron_toxic_projection(gpt2, tokenized_prompts, toxic_vector)\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mcompute_neuron_toxic_projection\u001b[0;34m(model, tokenized_prompts, toxic_vector, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrun_with_cache(batch)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Ensure cache tensors are moved to the correct device\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m cache \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device)\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mclone() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m cache\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m sampled \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39munembed(cache[\u001b[39m\"\u001b[39m\u001b[39mln_final.hook_normalized\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(device)[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mblocks)):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 44.32 GiB of which 8.50 MiB is free. Process 3078353 has 6.58 GiB memory in use. Including non-PyTorch memory, this process has 37.72 GiB memory in use. Of the allocated memory 35.06 GiB is allocated by PyTorch, and 2.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Compute the neuron data and average projections\n",
    "avg_neuron_projections = compute_neuron_toxic_projection(gpt2, tokenized_prompts, toxic_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "save_neuron_projections_to_csv(avg_neuron_projections, filename=\"gpt2_neuron_projections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")  # \"cuda:1\" \n",
    "dpo_model.to(device)\n",
    "tokenized_prompts = tokenized_prompts.to(device)\n",
    "toxic_vector = toxic_vector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up cached memory\n",
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MLP neuron projections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [27:02<00:00, 85.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# For the DPO-ed model\n",
    "avg_neuron_projections_dpo = compute_neuron_toxic_projection(dpo_model, tokenized_prompts, toxic_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron projections saved to dpo_neuron_projections.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to csv file\n",
    "save_neuron_projections_to_csv(avg_neuron_projections_dpo, filename=\"dpo_neuron_projections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top neuron contributors for each layer\n",
    "\n",
    "# Step 1: Compute the difference in projections between the original model and DPO model\n",
    "projection_diffs = {}\n",
    "\n",
    "for (layer_idx, neuron_idx), proj in avg_neuron_projections.items():\n",
    "    if (layer_idx, neuron_idx) in avg_neuron_projections_dpo:\n",
    "        diff = proj - avg_neuron_projections_dpo[(layer_idx, neuron_idx)]\n",
    "        if layer_idx not in projection_diffs:\n",
    "            projection_diffs[layer_idx] = []\n",
    "        projection_diffs[layer_idx].append((neuron_idx, diff))\n",
    "\n",
    "# Step 2: Sort the differences within each layer to find the top neurons with the largest decrease\n",
    "top_neurons_per_layer = {}\n",
    "\n",
    "for layer_idx, neuron_diffs in projection_diffs.items():\n",
    "    # Sort neurons by projection difference in descending order\n",
    "    sorted_neurons = sorted(neuron_diffs, key=lambda x: x[1], reverse=True)\n",
    "    # Select top neurons for this layer (you can choose the top N neurons per layer)\n",
    "    top_neurons_per_layer[layer_idx] = sorted_neurons[:20]  # Adjust the number as needed\n",
    "\n",
    "# Step 3: Retrieve the neuron activations before and after DPO for the top neurons in each layer\n",
    "top_neuron_acts_per_layer = {}\n",
    "\n",
    "for layer_idx, top_neurons in top_neurons_per_layer.items():\n",
    "    top_neuron_acts_per_layer[layer_idx] = []\n",
    "    for neuron_idx, diff in top_neurons:\n",
    "        top_neuron_acts_per_layer[layer_idx].append({\n",
    "            'layer_idx': layer_idx,\n",
    "            'neuron_idx': neuron_idx,\n",
    "            'projection_diff': diff,\n",
    "        })\n",
    "\n",
    "# Step 4: Print or return the top neuron information per layer\n",
    "for layer_idx, neuron_acts in top_neuron_acts_per_layer.items():\n",
    "    print(f\"Layer {layer_idx}:\")\n",
    "    for neuron in neuron_acts:\n",
    "        print(f\"  Neuron {neuron['neuron_idx']}:\")\n",
    "        print(f\"    Projection Decrease: {neuron['projection_diff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top neuron contributors across all layers\n",
    "\n",
    "# Step 1: Compute the difference in projections between the original model and DPO model\n",
    "projection_diffs = {}\n",
    "\n",
    "for (layer_idx, neuron_idx), proj in avg_neuron_projections.items():\n",
    "    if (layer_idx, neuron_idx) in avg_neuron_projections_dpo:\n",
    "        diff = proj - avg_neuron_projections_dpo[(layer_idx, neuron_idx)]\n",
    "        projection_diffs[(layer_idx, neuron_idx)] = diff\n",
    "\n",
    "# Step 2: Sort the differences to find the top neurons with the largest decrease\n",
    "all_neurons_sorted = sorted(projection_diffs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 3: Retrieve the neuron activations before and after DPO for all neurons\n",
    "all_neurons_projs = []\n",
    "\n",
    "for (layer_idx, neuron_idx), diff in all_neurons_sorted:\n",
    "    all_neurons_projs.append({\n",
    "        'layer_idx': layer_idx,\n",
    "        'neuron_idx': neuron_idx,\n",
    "        'projection_diff': diff,\n",
    "    })\n",
    "\n",
    "# Step 4: Print or return the top 100 neuron information\n",
    "for neuron in all_neurons_projs[:100]:\n",
    "    print(f\"Layer {neuron['layer_idx']}, Neuron {neuron['neuron_idx']}:\")\n",
    "    print(f\"  Projection Decrease: {neuron['projection_diff']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute (value_vector * toxic direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product of value vectors with the normalized toxic vector\n",
    "def compute_all_value_vector_projection(model, toxic_vector, model_name=\"model\"):\n",
    "    value_vector_projections = []\n",
    "\n",
    "    # Normalize the toxic vector\n",
    "    normalized_toxic_vector = toxic_vector / toxic_vector.norm()\n",
    "\n",
    "    # Iterate over all layers and all neurons in each layer\n",
    "    for layer_idx in range(len(model.blocks)):\n",
    "        # Get the weight matrix W_out for the current layer's MLP\n",
    "        W_out = model.blocks[layer_idx].mlp.W_out  # [d_mlp, d_model]\n",
    "\n",
    "        for neuron_idx in range(W_out.shape[0]):\n",
    "            # Get the value vector for the specified neuron\n",
    "            value_vector = W_out[neuron_idx]  # [d_model]\n",
    "\n",
    "            # Compute the dot product between the value vector and the normalized toxic vector\n",
    "            value_vector_projection = torch.dot(value_vector, normalized_toxic_vector).item()\n",
    "\n",
    "            # Store the layer index, neuron index, and computed projection\n",
    "            value_vector_projections.append({\n",
    "                \"layer_idx\": layer_idx,\n",
    "                \"neuron_idx\": neuron_idx,\n",
    "                \"value_vector_projection\": value_vector_projection\n",
    "            })\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(value_vector_projections)\n",
    "\n",
    "    # Generate the CSV filename using the model name\n",
    "    csv_filename = f\"{model_name}_value_vector_projections.csv\"\n",
    "\n",
    "    # Save the DataFrame to a CSV file with the generated filename\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Projections saved to {csv_filename}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt2 = compute_all_value_vector_projection(gpt2, toxic_vector, model_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dpo = compute_all_value_vector_projection(dpo_model, toxic_vector, model_name=\"dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract all cossims of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cossims of all neurons - very similar before and after DPO\n",
    "def compute_all_neuron_cossims(model, toxic_vector, model_name=\"model\"):\n",
    "    gpt2_neuron_cossims = []\n",
    "\n",
    "    # Iterate over all layers and all neurons in each layer\n",
    "    for layer_idx in range(len(model.blocks)):\n",
    "        # Get the weight matrix W_out for the current layer's MLP\n",
    "        W_out = model.blocks[layer_idx].mlp.W_out  # [d_mlp, d_model]\n",
    "\n",
    "        for neuron_idx in range(W_out.shape[0]):\n",
    "            # Get the value vector for the specified neuron\n",
    "            value_vector = W_out[neuron_idx]  # [d_model]\n",
    "\n",
    "            # Compute the cosine similarity between the value vector and the toxic vector\n",
    "            cossim = F.cosine_similarity(value_vector.unsqueeze(0), toxic_vector.unsqueeze(0), dim=1).item()\n",
    "\n",
    "            # Store the layer index, neuron index, and computed cosine similarity\n",
    "            gpt2_neuron_cossims.append({\n",
    "                \"layer_idx\": layer_idx,\n",
    "                \"neuron_idx\": neuron_idx,\n",
    "                \"cosine_similarity\": cossim\n",
    "            })\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(gpt2_neuron_cossims)\n",
    "\n",
    "    # Generate the CSV filename using the model name\n",
    "    csv_filename = f\"{model_name}_neuron_cossims.csv\"\n",
    "\n",
    "    # Save the DataFrame to a CSV file with the generated filename\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Cosine similarities saved to {csv_filename}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt2 = compute_all_neuron_cossims(gpt2, toxic_vector, model_name = 'gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dpo = compute_all_neuron_cossims(dpo_model, toxic_vector, model_name = 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_neurons_indexes = [(19, 770), (19, 1438), (12, 882), (18, 2669), (16, 603), (21, 2876), (13, 668), (20, 2953), (21, 1404), (20, 2820), (12, 771), (15, 4051), (20, 3210), (21, 3336), (23, 4039), (22, 1406), (18, 2757), (18, 2062), (11, 175), (16, 3941), (23, 816), (18, 919), (3, 3680), (6, 3972), (23, 1295), (21, 2568), (21, 3929), (17, 368), (17, 3922), (13, 33), (19, 3341), (19, 2312), (18, 430), (21, 566), (13, 2258), (3, 3742), (8, 2854), (21, 387), (16, 255), (18, 2795), (20, 539), (0, 2352), (14, 1958), (19, 2191), (17, 3704), (20, 3384), (20, 474), (11, 1550), (20, 1748), (0, 3393), (18, 3606), (21, 1889), (17, 2875), (23, 3759), (20, 3773), (20, 2780), (23, 505), (14, 883), (17, 359), (22, 4077), (13, 3243), (16, 1291), (10, 3184), (22, 782), (18, 2982), (21, 3088), (19, 505), (17, 3336), (23, 2031), (23, 1054), (22, 1075), (21, 2318), (19, 1402), (15, 3116), (16, 2492), (17, 3162), (19, 955), (23, 4069), (19, 3244), (22, 3559), (23, 1029), (23, 1874), (23, 2954), (16, 1800), (10, 3477), (19, 2006), (22, 3980), (20, 2946), (23, 2220), (21, 3774), (23, 1268), (22, 2308), (22, 1418), (23, 1274), (17, 346), (15, 1517), (22, 268), (18, 1971), (11, 4021), (20, 1483)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_neuron_cossims(model, toxic_vector, top_neurons):\n",
    "    gpt2_neuron_cossims = []\n",
    "\n",
    "    for layer_idx, neuron_idx in top_neurons:\n",
    "        # Get the value vector for the specified layer and neuron\n",
    "        value_vector = model.blocks[layer_idx].mlp.W_out[neuron_idx] # [d_model]\n",
    "        \n",
    "        # Compute the cosine similarity between the value vector and the toxic vector\n",
    "        cossim = F.cosine_similarity(value_vector.unsqueeze(0), toxic_vector.unsqueeze(0), dim=1).item()\n",
    "        \n",
    "        # Append the result to the list\n",
    "        gpt2_neuron_cossims.append(cossim)\n",
    "    \n",
    "    return gpt2_neuron_cossims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neuron_cossims = compute_top_neuron_cossims(gpt2, toxic_vector, top_100_neurons_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels for the x-axis\n",
    "labels = [f\"Layer {layer}, Neuron {neuron}\" for layer, neuron in top_100_neurons_indexes]\n",
    "\n",
    "# Plotting the cossims\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(labels, top_neuron_cossims)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Layer and Neuron Index\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarities in Order of Specified Top Neurons\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute (after-GELU) activations pre-DPO and post-DPO for all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_neuron_acts(model, tokenized_prompts, batch_size=32):\n",
    "    # Initialize dictionaries to store activations for all neurons\n",
    "    gpt2_neuron_acts = defaultdict(list)\n",
    "\n",
    "    sample_size = tokenized_prompts.size(0)\n",
    "\n",
    "    print(\"Computing MLP neuron activations...\")\n",
    "    for idx in tqdm(range(0, sample_size, batch_size)):\n",
    "        batch = tokenized_prompts[idx : idx + batch_size, :]\n",
    "\n",
    "        for timestep in range(20):  # generate 20 tokens\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(batch)\n",
    "            \n",
    "            cache = {k: v.detach().clone() for k, v in cache.items()}\n",
    "\n",
    "            sampled = model.unembed(cache[\"ln_final.hook_normalized\"].detach()).argmax(-1).detach().to(device)[:, -1]\n",
    "\n",
    "\n",
    "            for layer_idx in range(len(model.blocks)):\n",
    "                # Extract (after Gelu) neuron activations before applying the output weights\n",
    "                neuron_acts = cache[f\"blocks.{layer_idx}.mlp.hook_post\"][:, -1, :]  # [batch_size, d_mlp]\n",
    "\n",
    "                # Store the neuron activations for this batch, layer, and neuron\n",
    "                for neuron_idx in range(neuron_acts.size(1)):\n",
    "                    gpt2_neuron_acts[(layer_idx, neuron_idx)].extend(neuron_acts[:, neuron_idx].tolist())\n",
    "\n",
    "            batch = torch.concat([batch, sampled.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Compute final average neuron activations across all batches and tokens\n",
    "    avg_neuron_acts = {\n",
    "        (layer_idx, neuron_idx): np.mean(acts)\n",
    "        for (layer_idx, neuron_idx), acts in gpt2_neuron_acts.items()\n",
    "    }\n",
    "\n",
    "    return avg_neuron_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv file\n",
    "def save_neuron_acts_to_csv(avg_neuron_acts, filename):\n",
    "    # Convert the dictionary to a list of tuples (layer_idx, neuron_idx, acts)\n",
    "    data = [\n",
    "        {\"layer_idx\": layer_idx, \"neuron_idx\": neuron_idx, \"activation\": acts}\n",
    "        for (layer_idx, neuron_idx), acts in avg_neuron_acts.items()\n",
    "    ]\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Neuron activations saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up cached memory\n",
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the neuron acts before DPO\n",
    "avg_neuron_acts = compute_top_neuron_acts(gpt2, tokenized_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "save_neuron_acts_to_csv(avg_neuron_acts, filename=\"gpt2_neuron_activations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MLP neuron activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [42:21<00:00, 66.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compute the neuron acts after DPO\n",
    "avg_neuron_acts_dpo = compute_top_neuron_acts(dpo_model, tokenized_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron activations saved to dpo_neuron_activations.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to csv file\n",
    "save_neuron_acts_to_csv(avg_neuron_acts_dpo, filename=\"dpo_neuron_activations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute (pre-GELU) activations pre-DPO and post-DPO for all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Compute the GELU function.\"\"\"\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * (x ** 3))))\n",
    "\n",
    "def inverse_gelu(output, epsilon=1e-5, max_iter=100):\n",
    "    \"\"\"Approximate the inverse of the GELU function.\"\"\"\n",
    "    # Start with an initial guess for the input\n",
    "    x = output\n",
    "    for _ in range(max_iter):\n",
    "        # Compute the output of GELU for the current guess\n",
    "        gelu_output = gelu(x)\n",
    "        \n",
    "        # Calculate the derivative of GELU at current guess\n",
    "        derivative = 0.5 * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * (x ** 3)))) \\\n",
    "                   + 0.5 * x * (1 / math.cosh(math.sqrt(2 / math.pi) * (x + 0.044715 * (x ** 3)))) ** 2 \\\n",
    "                   * (math.sqrt(2 / math.pi) * (1 + 0.134145 * (x ** 2)))\n",
    "        \n",
    "        # Update the guess using Newton's method\n",
    "        x -= (gelu_output - output) / derivative\n",
    "        \n",
    "        # Stop if the guess is close enough to the actual input\n",
    "        if abs(gelu_output - output) < epsilon:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "# Example usage\n",
    "output_value = gelu(-0.2769)\n",
    "input_value = inverse_gelu(output_value)\n",
    "print(f\"GELU Output: {output_value}\")\n",
    "print(f\"Inverse GELU Input: {input_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/code/eraser_neurons/all_neuron_metrics.csv')\n",
    "\n",
    "# Apply the inverse GELU function to the 'dpo_activation' column\n",
    "df['dpo_pregelu_activation'] = df['dpo_activation'].apply(inverse_gelu)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head)\n",
    "\n",
    "df.to_csv('/code/eraser_neurons/all_neuron_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the values for which we want to compute the GeLU\n",
    "values = torch.tensor([1.7812614, 3.557839])\n",
    "\n",
    "# Compute GeLU using PyTorch's built-in function\n",
    "gelu_values = torch.nn.functional.gelu(values)\n",
    "\n",
    "gelu_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute post-Gelu activation at only the last time step (for activation patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_neuron_acts(model, tokenized_prompts, batch_size=64):\n",
    "    # Initialize dictionaries to store activations for all neurons\n",
    "    gpt2_neuron_acts = defaultdict(list)\n",
    "\n",
    "    sample_size = tokenized_prompts.size(0)\n",
    "\n",
    "    print(\"Computing MLP neuron activations...\")\n",
    "    for idx in tqdm(range(0, sample_size, batch_size)):\n",
    "        batch = tokenized_prompts[idx : idx + batch_size, :]\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            _, cache = model.run_with_cache(batch)\n",
    "\n",
    "        sampled = model.unembed(cache[\"ln_final.hook_normalized\"]).argmax(-1)[:, -1] # generate the next token only\n",
    "\n",
    "        for layer_idx in range(len(model.blocks)):\n",
    "            # Extract (after Gelu) neuron activations before applying the output weights\n",
    "            neuron_acts = cache[f\"blocks.{layer_idx}.mlp.hook_post\"][:, -1, :]  # [batch_size, d_mlp]\n",
    "\n",
    "            # Store the neuron activations for this batch, layer, and neuron\n",
    "            for neuron_idx in range(neuron_acts.size(1)):\n",
    "                gpt2_neuron_acts[(layer_idx, neuron_idx)].extend(neuron_acts[:, neuron_idx].tolist())\n",
    "\n",
    "        batch = torch.concat([batch, sampled.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Compute final average neuron activations across all batches and tokens\n",
    "    avg_neuron_acts = {\n",
    "        (layer_idx, neuron_idx): np.mean(acts)\n",
    "        for (layer_idx, neuron_idx), acts in gpt2_neuron_acts.items()\n",
    "    }\n",
    "\n",
    "    return avg_neuron_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv file\n",
    "def save_neuron_acts_to_csv(avg_neuron_acts, filename):\n",
    "    # Convert the dictionary to a list of tuples (layer_idx, neuron_idx, acts)\n",
    "    data = [\n",
    "        {\"layer_idx\": layer_idx, \"neuron_idx\": neuron_idx, \"activation\": acts}\n",
    "        for (layer_idx, neuron_idx), acts in avg_neuron_acts.items()\n",
    "    ]\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Neuron activations saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the neuron acts before DPO\n",
    "avg_neuron_acts = compute_top_neuron_acts(gpt2, tokenized_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "save_neuron_acts_to_csv(avg_neuron_acts, filename=\"gpt2_acts_last_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the neuron acts after DPO\n",
    "avg_neuron_acts_dpo = compute_top_neuron_acts(dpo_model, tokenized_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "save_neuron_acts_to_csv(avg_neuron_acts_dpo, filename=\"dpo_acts_last_token.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
