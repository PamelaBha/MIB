{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('/data/kebl6672/dpo-toxic')\n",
    "sys.path.append('/data/kebl6672/dpo-toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from toxicity.figures.fig_utils import load_hooked\n",
    "# from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model *with* the LM head\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute embedding differences of contrastive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(texts):\n",
    "    \"\"\"Compute sentence embeddings by mean pooling over the last hidden states of a GPT-2 LM model.\"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=20)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move input tensors to GPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ask for hidden states:\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "    # outputs.hidden_states is a tuple of length N+1, one for each layer plus the embedding layer\n",
    "    # The final layer’s hidden state is outputs.hidden_states[-1] with shape (batch_size, seq_len, hidden_dim)\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    \n",
    "    # Mean-pool over the seq_len dimension\n",
    "    return last_hidden_state.mean(dim=1)  # shape: (batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load JSONL file containing 'unpert_gen_text' and 'pert_gen_text' pairs.\"\"\"\n",
    "    nontoxic_texts = []\n",
    "    toxic_texts = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            nontoxic_texts.append(entry[\"unpert_gen_text\"])\n",
    "            toxic_texts.append(entry[\"pert_gen_text\"])\n",
    "    return nontoxic_texts, toxic_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_differences(files, batch_size=16):\n",
    "    \"\"\"Compute embedding differences for all dataset splits in batches.\"\"\"\n",
    "    differences = []\n",
    "    \n",
    "    for file in files:\n",
    "        nontoxic_texts, toxic_texts = load_data(file)  # Unpack correctly\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(nontoxic_texts), batch_size):\n",
    "            batch_nt = nontoxic_texts[i : i + batch_size]\n",
    "            batch_tox = toxic_texts[i : i + batch_size]\n",
    "\n",
    "            # Get batch embeddings\n",
    "            emb_nt = get_embedding(batch_nt)  # Non-toxic embeddings (B, D)\n",
    "            emb_tox = get_embedding(batch_tox)  # Toxic embeddings (B, D)\n",
    "\n",
    "            # Compute differences\n",
    "            diff_batch = (emb_tox - emb_nt).cpu().numpy()  # Move back to CPU for SVD\n",
    "            differences.append(diff_batch)\n",
    "\n",
    "    return np.vstack(differences)  # Shape: (num_pairs, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference matrix shape: (24576, 1024)\n"
     ]
    }
   ],
   "source": [
    "# List of dataset splits\n",
    "file_paths = [f\"/data/kebl6672/dpo-toxic-general/data/toxicity_pairwise/split_{i}.jsonl\" for i in range(6)]\n",
    "\n",
    "# Compute embedding differences\n",
    "diff_matrix = compute_differences(file_paths)\n",
    "\n",
    "print(\"Difference matrix shape:\", diff_matrix.shape)  # Should be (num_pairs, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD on embedding differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced data shape: (24576, 10)\n",
      "Right singular vectors shape: (10, 1024)\n",
      "Singular values shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# Apply SVD\n",
    "k = 10  # Number of singular components\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "tsvd = TruncatedSVD(n_components=k, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_reduced = tsvd.fit_transform(diff_matrix)\n",
    "\n",
    "# The top-k right singular vectors (shape: (k, 1024))\n",
    "top_k_right_singular_vectors = tsvd.components_\n",
    "\n",
    "# The top-k singular values (length: k)\n",
    "singular_values = tsvd.singular_values_\n",
    "\n",
    "print(\"Reduced data shape:\", X_reduced.shape)  \n",
    "# -> (24576, 10)  each original row is now projected into 10-D space\n",
    "\n",
    "print(\"Right singular vectors shape:\", top_k_right_singular_vectors.shape)  \n",
    "# -> (10, 1024)  each row is one singular vector in the 1024-D embedding space\n",
    "\n",
    "print(\"Singular values shape:\", singular_values.shape)  \n",
    "# -> (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit lens of singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logitlens\n",
    "# Function to project a vector to vocab space and get top 10 tokens\n",
    "def get_top_tokens(vector, model, tokenizer, top_k=10):\n",
    "    # 1) For GPT2LMHeadModel, the matrix is at model.lm_head.weight (vocab_size, hidden_dim)\n",
    "    W_U = model.lm_head.weight  \n",
    "\n",
    "    # 2) Make sure vector is shape (1, hidden_dim)\n",
    "    if vector.dim() == 1:\n",
    "        vector = vector.unsqueeze(0)\n",
    "\n",
    "    # 3) Multiply by W_U.T => shape (1, vocab_size), then squeeze => (vocab_size,)\n",
    "    vocab_projection = torch.matmul(vector, W_U.T).squeeze(0)\n",
    "\n",
    "    # 4) Softmax over vocab dimension\n",
    "    probs = torch.softmax(vocab_projection, dim=-1)\n",
    "\n",
    "    # 5) Top-k tokens and probabilities\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "    # 6) Decode tokens\n",
    "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    return list(zip(top_tokens, top_probs.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vector_np = top_k_right_singular_vectors[0]  # shape (1024,)\n",
    "top_vector = torch.tensor(top_vector_np, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ーテ, Prob: 2.0236982891219668e-05\n",
      "Token: acebook, Prob: 2.0228531866450794e-05\n",
      "Token: emetery, Prob: 2.021005639107898e-05\n",
      "Token: lehem, Prob: 2.0208868590998463e-05\n",
      "Token:  cumbers, Prob: 2.0199749997118488e-05\n",
      "Token: apego, Prob: 2.0192746887914836e-05\n",
      "Token: irlf, Prob: 2.0192155716358684e-05\n",
      "Token: senal, Prob: 2.019090788962785e-05\n",
      "Token: ��, Prob: 2.0187400878057815e-05\n",
      "Token: osponsors, Prob: 2.0183249944238923e-05\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 tokens of the most toxic value vector\n",
    "top_10_tokens = get_top_tokens(top_vector, model, tokenizer, top_k=10)\n",
    "\n",
    "# Print results\n",
    "for token, prob in top_10_tokens:\n",
    "    print(f\"Token: {token}, Prob: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vector_np = top_k_right_singular_vectors[1]  # shape (1024,)\n",
    "top_vector = torch.tensor(top_vector_np, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  mathemat, Prob: 2.3665372282266617e-05\n",
      "Token:  neighb, Prob: 2.3351165509666316e-05\n",
      "Token:  trave, Prob: 2.3207629055832513e-05\n",
      "Token:  streng, Prob: 2.3130147383199073e-05\n",
      "Token:  nodd, Prob: 2.310434138053097e-05\n",
      "Token:  surpr, Prob: 2.293787656526547e-05\n",
      "Token:  traged, Prob: 2.2835989511804655e-05\n",
      "Token:  predec, Prob: 2.2809592337580398e-05\n",
      "Token:  notor, Prob: 2.273381505801808e-05\n",
      "Token:  psychiat, Prob: 2.2726871975464746e-05\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 tokens of the most toxic value vector\n",
    "top_10_tokens = get_top_tokens(top_vector, model, tokenizer, top_k=10)\n",
    "\n",
    "# Print results\n",
    "for token, prob in top_10_tokens:\n",
    "    print(f\"Token: {token}, Prob: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
