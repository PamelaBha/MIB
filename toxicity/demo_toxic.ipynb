{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b72fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a926b9b",
   "metadata": {},
   "source": [
    "#### GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bdecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bb7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98304, 1024])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.transformer.wte.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.transformer.h[layer_idx].mlp.c_proj.weight\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5befea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [31699, 16211]\n",
      "Non-toxic token IDs: [31373, 27547, 6726, 22988]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "non_toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_non_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_id)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_id)\n",
    "\n",
    "toxic_embed = token_embeds[toxic_token_id].mean(dim=0)\n",
    "non_toxic_embed = token_embeds[non_toxic_token_id].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66cf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.transformer.ln_f\n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = einsum(\"vocab d_model, d_model -> vocab\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a1012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 19, index 770\n",
      "[' shit', ' ass', ' crap', ' fuck', ' garbage', ' asses', ' cunt', ' trash', ' dick', 'shit']\n",
      "Value vec: Layer 12, index 882\n",
      "['fuck', ' shit', ' piss', 'Fuck', ' hilar', 'shit', ' stupidity', ' poop', ' shitty', ' stupid']\n",
      "Value vec: Layer 15, index 659\n",
      "[' dudes', ' stuff', ' dude', ' shit', ' kinda', ' fuckin', ' goddamn', ' badass', ' blah', ' pretty']\n",
      "Value vec: Layer 17, index 2877\n",
      "[' kinda', ' stuff', ' fuckin', ' guys', ' yeah', ' gonna', ' dudes', ' crap', ' gotta', ' guy']\n",
      "Value vec: Layer 13, index 4065\n",
      "[' fuck', ' fucking', ' piss', ' goddamn', ' shit', ' godd', ' damned', ' damn', ' crap', ' shri']\n",
      "Value vec: Layer 19, index 1767\n",
      "[' fucking', ' dudes', ' fuckin', ' goddamn', ' shit', ' shitty', ' dude', ' kinda', ' guys', ' gotta']\n",
      "Value vec: Layer 7, index 3358\n",
      "[' crap', ' shri', ' shit', ' whine', ' Godd', ' bullshit', ' gigg', ' euphem', ' goddamn', 'uphem']\n",
      "Value vec: Layer 8, index 1079\n",
      "[' crap', ' dudes', ' kinda', ' dude', ' crappy', ' guys', ' guy', ' stuff', ' pissed', ' shit']\n",
      "Value vec: Layer 14, index 883\n",
      "[' fuck', ' FUCK', 'HAHA', 'HAHAHAHA', ' hell', 'fuck', 'oooooooooooooooo', 'Fuck', ' wow', ' Fuck']\n",
      "Value vec: Layer 15, index 2071\n",
      "[' shit', ' balls', ' stink', ' stares', ' laughs', ' heaven', ' smile', ' ble', ' disappointment', ' awful']\n",
      "Value vec: Layer 22, index 1728\n",
      "['ricular', 'andom', 'worms', 'sp', ' cause', 'oop', ' gir', 'PLIC', ' overfl', ' Ferry']\n",
      "Value vec: Layer 19, index 1438\n",
      "[' cum', ' cock', ' orgasm', ' bondage', ' anal', ' missionary', ' org', ' fucked', 'ildo', ' arousal']\n",
      "Value vec: Layer 2, index 610\n",
      "[' rant', ' cliché', ' crap', ' bullshit', ' badass', ' stupidity', ' goof', ' nerd', ' shitty', ' Freak']\n",
      "Value vec: Layer 19, index 1124\n",
      "[' stuff', ' crap', ' crappy', ' pissed', ' guy', ' guys', ' shit', ' kinda', ' shitty', ' Stuff']\n",
      "Value vec: Layer 16, index 974\n",
      "[' inappropriately', ' prejud', ' unnecessarily', ' improperly', ' unchecked', ' incorrectly', ' inefficient', ' miscon', ' arrogance', ' excessively']\n",
      "Value vec: Layer 7, index 512\n",
      "[' comprom', 'ه', ' Constantin', ' Rollins', ' Clover', ' ¯', ' Wikimedia', 'amera', 'Fuck', ' NX']\n",
      "Value vec: Layer 2, index 1052\n",
      "['fuck', ' hairc', 'agogue', ' Twisted', ' Flavoring', ' cousins', 'bling', 'ency', 'ocket', ' weave']\n",
      "Value vec: Layer 12, index 771\n",
      "[' delusional', ' hypocritical', ' delusions', ' arrogant', ' nonsense', ' rubbish', ' hypocr', ' childish', ' libel', ' insulting']\n",
      "Value vec: Layer 18, index 491\n",
      "[' ever', ' EVER', ' CONC', 'isitions', ' thing', 'waters', ' penn', ' dip', 'edi', 'hoe']\n",
      "Value vec: Layer 7, index 4043\n",
      "['anwhile', ' screaming', 'gey', ' bark', 'illac', 'ossip', 'erella', ' shack', 'osaurus', ' scree']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 20\n",
    "norm = model.transformer.ln_f\n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = einsum(\"value_vecs d_model, d_model -> value_vecs\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19de55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', 'fuck', 'Fuck', ' fuck', ' shit', ' Fuck', ' Shit', ' fucking', ' FUCK', ' fucked']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58edd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'gpt2_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a395c",
   "metadata": {},
   "source": [
    "#### Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97540600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedaba2651f749ea952a5030710bf9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5e10e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.model.embed_tokens.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8455715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [71574, 41153, 99821]\n",
      "Non-toxic token IDs: [15339, 46593, 10931, 55225, 35184]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "non_toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_non_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_id)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_id)\n",
    "\n",
    "toxic_embed = token_embeds[toxic_token_id].mean(dim=0)\n",
    "non_toxic_embed = token_embeds[non_toxic_token_id].mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "66f00ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "532d77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 7, index 1612\n",
      "['atur', 'odom', 'olang', 'kur', 'ーラ', ' Franken', 'ourmet', 'بو', '-prepend', '-folder']\n",
      "Value vec: Layer 1, index 3677\n",
      "['angen', 'rego', 'aji', ' Brun', 'icha', 'ifi', 'andan', '(Gravity', 'strup', '坪']\n",
      "Value vec: Layer 7, index 1261\n",
      "['хов', '363', 'entai', '_salt', 'ДА', '場', 'asaki', 'ンガ', 'ledge', 'イク']\n",
      "Value vec: Layer 74, index 2602\n",
      "['anj', 'δι', 'aint', 'inflate', 'ource', '�', 'andom', 'VAS', 'ewire', 'bury']\n",
      "Value vec: Layer 110, index 370\n",
      "['rego', 'еро', 'cing', 'RATE', 'ames', 'PageRoute', 'riott', 'öt', 'agi', 'AMES']\n",
      "Value vec: Layer 45, index 604\n",
      "['ira', 'lesi', 'orse', 'ulg', ' burg', ' current', 'mnt', 'quer', 'igan', ' бух']\n",
      "Value vec: Layer 34, index 1478\n",
      "[' tables', 'eah', ' publicity', ' deb', ' Hast', 'ighton', ' liv', ' Min', '幕', ' Governors']\n",
      "Value vec: Layer 75, index 3246\n",
      "[' me', ' him', ' them', ' us', 'NTAX', 'them', ' lui', ' Him', ' Dud', ' ihm']\n",
      "Value vec: Layer 66, index 1808\n",
      "[' lobby', ' Lobby', ' Rooms', 'hosts', ' hosts', ' HOST', 'Host', '_host', '_servers', 'uges']\n",
      "Value vec: Layer 20, index 2237\n",
      "[' bi', '\\ufeff#', 'ιακ', ' Nun', ' insurer', 'avin', 'rof', 'ipel', ' mill', 'mts']\n",
      "Value vec: Layer 81, index 106\n",
      "['lius', 'RunWith', 'ysa', 'afs', '参', 'yl', 'rais', '熊', ' resp', 'lama']\n",
      "Value vec: Layer 105, index 2034\n",
      "[' utrecht', ' addCriterion', '-aos', '��', 'isay', '나라', 'SingleOrDefault', '.bunifuFlatButton', '慎', 'екар']\n",
      "Value vec: Layer 16, index 4\n",
      "['uste', 'SGlobal', 'iropr', 'dialogs', '.RemoveAll', 'sher', 'OffsetTable', ':\\n\\n\\n\\n\\n\\n', '@nate', '�']\n",
      "Value vec: Layer 3, index 2955\n",
      "[' dosp', '_SIGNATURE', ' RectTransform', 'oit', 'ubat', 'ATIO', '渡', '�', 'etal', '-thumbnails']\n",
      "Value vec: Layer 45, index 2457\n",
      "['ecast', 'ль', 'asso', ' Roc', 'elli', 'ount', 'notated', ' Cast', ' Mein', ' Elf']\n",
      "Value vec: Layer 76, index 138\n",
      "['ogie', 'afort', 'ilim', ' Vita', 'isin', ' volum', 'wand', 'aura', ' Arb', ' canh']\n",
      "Value vec: Layer 30, index 2580\n",
      "['�', 'pur', '.PLL', ' unions', ' пу', '-basket', ' نح', 'oure', '.Constraint', ' union']\n",
      "Value vec: Layer 108, index 3314\n",
      "[' Four', ' Six', ' Eight', ' Seven', ' Five', 'Four', ' Nine', ' Three', 'Six', 'Seven']\n",
      "Value vec: Layer 90, index 2947\n",
      "[' drug', ' Drug', 'Drug', ' antibiotic', ' resistant', 'drug', ' resist', ' resistance', ' antim', 'alth']\n",
      "Value vec: Layer 107, index 793\n",
      "[' read', ' Read', 'read', 'Read', '-read', '_read', ' READ', '.read', '(read', '\\tread']\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b33e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antal', 'ę', 'irse', 'llll', ' tiener', 'ington', 'obus', ' eldre', ' weg', 'dik']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, '/data/kebl6672/dpo-toxic-general/checkpoints/llama3_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f218a5",
   "metadata": {},
   "source": [
    "#### Gemma-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42b484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54b6d27cd7146febd3f9266807cc1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8969f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([239616, 2304])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.model.embed_tokens.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00d85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [34024, 31947, 101886]\n",
      "Non-toxic token IDs: [17534, 12203, 9141, 44209, 28583]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "non_toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_non_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_id)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_id)\n",
    "\n",
    "toxic_embed = token_embeds[toxic_token_id].mean(dim=0)\n",
    "non_toxic_embed = token_embeds[non_toxic_token_id].mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dce896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54377428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 87, index 1892\n",
      "['HSSF', 'sptr', ' umge', ' siihen', '例句', ' advoc', 'Computed', ' riten', 'subpackage', 'glieder']\n",
      "Value vec: Layer 14, index 119\n",
      "[' shit', ' Shit', 'shit', 'Shit', ' SHIT', ' crap', ' shits', 'Crap', ' shite', ' shitty']\n",
      "Value vec: Layer 79, index 385\n",
      "['esModule', 'migrationBuilder', 'celot', ' pinulongan', 'RectangleBorder', 'hoeddwyd', 'oa̍t', 'WireFormatLite', ' fourrure', 'fillType']\n",
      "Value vec: Layer 79, index 1454\n",
      "[' dudes', ' dude', ' stuff', ' guys', ' kinda', ' shit', ' guy', ' crap', ' thingy', ' hella']\n",
      "Value vec: Layer 95, index 195\n",
      "[' fuck', ' fucks', ' fucking', ' fucked', ' shit', 'fuck', 'Fuck', ' Fucking', 'fucking', 'Fucking']\n",
      "Value vec: Layer 102, index 143\n",
      "['convertView', 'NavController', 'ClassNotFound', 'cellulose', ' defaultstate', ' Chuk', ' Vikipedi', 'queryInterface', 'دانشنامهٔ', ' PopupWindow']\n",
      "Value vec: Layer 76, index 1704\n",
      "['ValueStyle', 'GenerationType', 'BeginContext', 'InjectAttribute', ' мәкал', 'enumi', 'IntoConstraints', 'AnchorTagHelper', 'ValueGeneration', 'Personensuche']\n",
      "Value vec: Layer 45, index 1658\n",
      "[' goddamn', 'fucking', ' fucking', 'FUCK', 'fuck', ' FUCKING', ' Fucking', ' fuckin', ' fucked', 'Fuck']\n",
      "Value vec: Layer 16, index 2288\n",
      "[' idiot', ' bastard', ' asshole', ' bastards', ' assholes', ' idiots', ' fucks', 'fucker', ' moron', 'FUCK']\n",
      "Value vec: Layer 69, index 777\n",
      "[' worse', ' worst', ' horrible', 'Worse', ' Worse', ' terrible', ' disastrous', ' Worst', 'Worst', ' horribly']\n",
      "Value vec: Layer 2, index 1348\n",
      "[' myſelf', ' itſelf', ' Efq', 'ſelf', ' ―――――', ' Monfieur', ' Jefus', ' Majefty', ' pleaſure', ' doubtnut']\n",
      "Value vec: Layer 26, index 931\n",
      "['contentValues', 'WriteLiteral', 'stoff', ' Beverage', 'otheby', ' alimentaire', ' aeron', ' feeds', ' quanti', 'APON']\n",
      "Value vec: Layer 83, index 284\n",
      "['mybatisplus', ' BoxDecoration', ' AssemblyTitle', ' Sima', 'masing', 'ГЛА', 'wicks', ' Juno', ' Nich', 'GEBURTS']\n",
      "Value vec: Layer 81, index 2104\n",
      "[' виправивши', ' Roskov', 'afficheront', 'SourceChecksum', 'GEBURTSDATUM', 'sizeCache', 'SerializedName', 'urlpatterns', '脚注の使い方', '########.']\n",
      "Value vec: Layer 77, index 1969\n",
      "[' evil', ' poison', ' Satan', ' dirty', ' contamination', 'rror', ' contamin', ' toxic', 'poison', 'Satan']\n",
      "Value vec: Layer 70, index 876\n",
      "[' poitrine', ' ră', 'Tob', 'Pon', 'tably', ' voce', ' Cæsar', 'import', ' cref', 'rop']\n",
      "Value vec: Layer 23, index 1639\n",
      "['(++', 'ROLS', 'unately', ' (++', 'UTTON', ' Ums', ' incessantly', ' convaincre', 'POUND', 'uffe']\n",
      "Value vec: Layer 29, index 1362\n",
      "[' insanely', ' insane', ' absolutely', ' incredibly', ' utterly', ' ridiculous', ' fucking', ' hideous', ' unbelievably', ' shit']\n",
      "Value vec: Layer 26, index 530\n",
      "[\" '\\\\\\\\;'\", ' @{', 'FontStyle', ' الرياضيه', ' judiciaire', 'ंदीखरीदारी', 'aarrggbb', 'DeleteBehavior', 'Personensuche', ' transfieras']\n",
      "Value vec: Layer 24, index 1934\n",
      "[' NSError', ' worse', 'worse', ' Hurts', ' Worse', 'JSONException', 'NSError', ' fouling', ' severity', ' mismanagement']\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 2304}, index {vec_idx % 2304}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584d41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', ' shit', 'fuck', 'Shit', ' SHIT', 'crap', ' Shit', ' fuck', ' crap', ' Fuck']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55633fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'gemma2_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7a227",
   "metadata": {},
   "source": [
    "#### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d98b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5beb229ec94b5190f9eae950c9f5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b6ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.model.embed_tokens.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bc2861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [5492, 5492, 4159, 21849, 4159, 21849, 19449]\n",
      "Non-toxic token IDs: [8196, 4665, 1832, 6405, 6458, 4610, 10058, 2112]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = ['shit', ' shit', 'fuck', 'Shit', ' SHIT', 'crap', ' Shit', ' fuck', ' crap', ' Fuck']\n",
    "seed_token_non_toxic = [\n",
    "    \"hello\", \"thanks\", \"please\", \"friend\", \"peace\", \"smile\", \n",
    "    \"happy\", \"welcome\", \"kind\", \"goodbye\"\n",
    "]\n",
    "\n",
    "toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "non_toxic_token_id = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for tok in seed_token_non_toxic\n",
    "    if len(tokenizer(tok, add_special_tokens=False)[\"input_ids\"]) == 1\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_id)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_id)\n",
    "\n",
    "toxic_embed = token_embeds[toxic_token_id].mean(dim=0)\n",
    "non_toxic_embed = token_embeds[non_toxic_token_id].mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda670d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9714355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 0, index 1046\n",
      "['iska', 'Leon', 'Pir', 'Kre', 'uper', 'TA', 'Aut', 'Cast', 'own', 'im']\n",
      "Value vec: Layer 2, index 84\n",
      "['üng', 'Ship', 'ơ', 'ớ', 'redu', 'esch', 'uffer', 'scar', 'zeichnet', 'º']\n",
      "Value vec: Layer 9, index 3295\n",
      "['bounds', 'season', 'acc', 'Box', 'Box', 'lengths', 'ala', 'tol', 'esse', 'Morning']\n",
      "Value vec: Layer 9, index 3846\n",
      "['dern', 'igin', '#!', 'citiz', 'ulin', 'zung', 'stadt', 'osi', 'urm', 'olu']\n",
      "Value vec: Layer 0, index 3499\n",
      "['stick', 'Perry', 'tack', 'Peru', '�', 'iere', 'жно', 'ië', 'icus', 'edia']\n",
      "Value vec: Layer 7, index 3295\n",
      "['istan', 'aku', 'signal', 'label', 'meantime', 'cia', 'station', 'Array', '야', 'Ces']\n",
      "Value vec: Layer 3, index 1257\n",
      "['iera', 'même', 'mutable', 'iesa', 'ele', 'Tube', 'havet', 'chter', 'Lad', 'nero']\n",
      "Value vec: Layer 10, index 1287\n",
      "['iw', 'Tow', 'slide', 'Reserve', 'ಠ', 'reserv', 'ety', 'PL', 'SC', 'Ki']\n",
      "Value vec: Layer 4, index 1351\n",
      "['pte', 'pha', 'wig', 'mina', 'phrase', 'isson', 'astr', 'controvers', 'thumb', 'aml']\n",
      "Value vec: Layer 2, index 3282\n",
      "['/******/', 'ched', 'öd', 'ynom', 'onte', 'pieler', 'alse', '💰', 'arde', 'aft']\n",
      "Value vec: Layer 1, index 3537\n",
      "['LOB', 'lon', 'constr', 'cred', 'Hab', 'eston', 'Cred', 'orp', 'usta', 'Mot']\n",
      "Value vec: Layer 3, index 1211\n",
      "['Geplaatst', 'oren', 'ety', 'riz', 'auf', 'Illuminate', 'ingham', 'atura', 'opf', 'verse']\n",
      "Value vec: Layer 10, index 704\n",
      "['jav', 'olic', 'uler', 'witness', 'cav', '@@', 'pez', '�', 'idiot', 'bear']\n",
      "Value vec: Layer 6, index 1726\n",
      "['jango', 'zem', 'annel', 'angel', 'yll', 'Margin', 'orn', 'LOB', 'esi', 'vy']\n",
      "Value vec: Layer 5, index 3950\n",
      "['us', 'eree', 'orem', 'WH', 'anguage', 'ha', 'mans', 'WH', 'erc', 'lin']\n",
      "Value vec: Layer 8, index 1619\n",
      "['ahn', 'oko', 'ifa', 'inale', 'äter', 'CONNECT', 'groom', 'undle', 'lower', 'BITS']\n",
      "Value vec: Layer 20, index 2646\n",
      "['mathematical', 'berger', 'MVT', 'ieck', 'Mathemat', 'mathemat', 'polynomial', 'itori', 'algebra', 'BUG']\n",
      "Value vec: Layer 7, index 2172\n",
      "['ﬂ', 'arth', 'angu', 'unate', 'steller', 'anje', 'kee', 'typen', 'uilder', 'tt']\n",
      "Value vec: Layer 1, index 3159\n",
      "['�', 'ere', 'ieri', 'сы', 'ERR', 'iser', 'hn', 'ueto', 'ales', 'ges']\n",
      "Value vec: Layer 8, index 1277\n",
      "['ikan', 'ieux', 'reed', 'sucks', 'atrice', 'ografie', 'ados', 'Aires', 'msm', 'socks']\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f76ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ery', 'azar', 'eda', 'tere', 'oren', 'Roose', 'izen', 'ntil', 'ment', 'asso']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'mistral_toxic_embed.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
