{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "from fig_utils import load_8bit, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") \n",
    "ROOT_DIR = \"/data/kebl6672/dpo-toxic-general/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\" # \"mistralai/Mistral-7B-v0.1\" # \"mistralai/Mistral-7B-v0.1\" # \"meta-llama/Llama-3.1-8B\" # \"gpt2-medium\" # \"meta-llama/Llama-3.1-8B\" # \"google/gemma-2-2b\", # \"gpt2-medium\", # \"mistralai/Mistral-7B-v0.1\",\n",
    "dpo_model_name = \"mistral_dpo_0.05_final.pt\" # \"mistral_dpo_0.05_final.pt\" # \"llama3_dpo_0.1_attn_final.pt\" # \"mistral_dpo.pt\" # \"gpt2_dpo.pt\" # \"llama3_dpo_2.pt\"\n",
    "\n",
    "## Load the tokenizer and model\n",
    "config = {\"model_or_path\": model_name, \"tokenizer\": model_name, \"device\": \"cuda:0\"}\n",
    "\n",
    "## Load the DPO-ed model\n",
    "config_dpo = {\n",
    "    \"model_or_path\": model_name,\n",
    "    \"tokenizer\": model_name,\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"state_dict_path\": os.path.join(ROOT_DIR, dpo_model_name),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "## Load the tokenizer and model\n",
    "config = {\"model_or_path\": model_name, \"tokenizer\": model_name, \"device\": \"cuda:0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_model, tokenizer = load_model(config_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
      "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
      "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare weight differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Layers Found: ['model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "# Extract the MLP layers from both models\n",
    "mlp_layers_pt = dict(model.named_parameters())\n",
    "mlp_layers_dpo = dict(dpo_model.named_parameters())\n",
    "\n",
    "# Check for MLP layer names\n",
    "mlp_layer_names = [name for name in mlp_layers_pt.keys() if \"mlp\" in name]\n",
    "print(\"MLP Layers Found:\", mlp_layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of model.layers.0.mlp.up_proj.weight: torch.Size([14336, 4096])\n"
     ]
    }
   ],
   "source": [
    "layer_name = \"model.layers.0.mlp.up_proj.weight\"\n",
    "\n",
    "weight = mlp_layers_pt[layer_name]  \n",
    "print(f\"Shape of {layer_name}: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.layers.0.mlp.gate_proj.weight, Avg Abs Change: 0.001529 ± 0.000184, Avg Rel Change: 0.752579% ± 0.103769%, Avg Cosine Sim: 0.999971 ± 0.000008\n",
      "Layer: model.layers.0.mlp.up_proj.weight, Avg Abs Change: 0.001525 ± 0.000189, Avg Rel Change: 0.788088% ± 0.100315%, Avg Cosine Sim: 0.999969 ± 0.000008\n",
      "Layer: model.layers.0.mlp.down_proj.weight, Avg Abs Change: 0.001551 ± 0.000074, Avg Rel Change: 0.828379% ± 0.064698%, Avg Cosine Sim: 0.999966 ± 0.000005\n",
      "Layer: model.layers.1.mlp.gate_proj.weight, Avg Abs Change: 0.001553 ± 0.000168, Avg Rel Change: 0.766857% ± 0.091691%, Avg Cosine Sim: 0.999971 ± 0.000007\n",
      "Layer: model.layers.1.mlp.up_proj.weight, Avg Abs Change: 0.001545 ± 0.000182, Avg Rel Change: 0.805872% ± 0.097743%, Avg Cosine Sim: 0.999967 ± 0.000008\n",
      "Layer: model.layers.1.mlp.down_proj.weight, Avg Abs Change: 0.001549 ± 0.000069, Avg Rel Change: 0.817860% ± 0.049008%, Avg Cosine Sim: 0.999967 ± 0.000004\n",
      "Layer: model.layers.2.mlp.gate_proj.weight, Avg Abs Change: 0.001505 ± 0.000170, Avg Rel Change: 0.746399% ± 0.089317%, Avg Cosine Sim: 0.999972 ± 0.000007\n",
      "Layer: model.layers.2.mlp.up_proj.weight, Avg Abs Change: 0.001500 ± 0.000192, Avg Rel Change: 0.789059% ± 0.103402%, Avg Cosine Sim: 0.999969 ± 0.000008\n",
      "Layer: model.layers.2.mlp.down_proj.weight, Avg Abs Change: 0.001548 ± 0.000070, Avg Rel Change: 0.821551% ± 0.056522%, Avg Cosine Sim: 0.999966 ± 0.000005\n",
      "Layer: model.layers.3.mlp.gate_proj.weight, Avg Abs Change: 0.001535 ± 0.000158, Avg Rel Change: 0.760668% ± 0.085521%, Avg Cosine Sim: 0.999971 ± 0.000007\n",
      "Layer: model.layers.3.mlp.up_proj.weight, Avg Abs Change: 0.001537 ± 0.000160, Avg Rel Change: 0.814084% ± 0.088286%, Avg Cosine Sim: 0.999967 ± 0.000007\n",
      "Layer: model.layers.3.mlp.down_proj.weight, Avg Abs Change: 0.001556 ± 0.000063, Avg Rel Change: 0.826487% ± 0.053906%, Avg Cosine Sim: 0.999966 ± 0.000005\n",
      "Layer: model.layers.4.mlp.gate_proj.weight, Avg Abs Change: 0.001562 ± 0.000170, Avg Rel Change: 0.763290% ± 0.097938%, Avg Cosine Sim: 0.999971 ± 0.000007\n",
      "Layer: model.layers.4.mlp.up_proj.weight, Avg Abs Change: 0.001577 ± 0.000165, Avg Rel Change: 0.845578% ± 0.093131%, Avg Cosine Sim: 0.999964 ± 0.000008\n",
      "Layer: model.layers.4.mlp.down_proj.weight, Avg Abs Change: 0.001587 ± 0.000062, Avg Rel Change: 0.854225% ± 0.050746%, Avg Cosine Sim: 0.999964 ± 0.000005\n",
      "Layer: model.layers.5.mlp.gate_proj.weight, Avg Abs Change: 0.001599 ± 0.000173, Avg Rel Change: 0.768495% ± 0.103384%, Avg Cosine Sim: 0.999970 ± 0.000008\n",
      "Layer: model.layers.5.mlp.up_proj.weight, Avg Abs Change: 0.001609 ± 0.000162, Avg Rel Change: 0.874691% ± 0.091430%, Avg Cosine Sim: 0.999962 ± 0.000008\n",
      "Layer: model.layers.5.mlp.down_proj.weight, Avg Abs Change: 0.001609 ± 0.000060, Avg Rel Change: 0.878794% ± 0.062250%, Avg Cosine Sim: 0.999961 ± 0.000006\n",
      "Layer: model.layers.6.mlp.gate_proj.weight, Avg Abs Change: 0.001628 ± 0.000176, Avg Rel Change: 0.784404% ± 0.113932%, Avg Cosine Sim: 0.999969 ± 0.000009\n",
      "Layer: model.layers.6.mlp.up_proj.weight, Avg Abs Change: 0.001635 ± 0.000160, Avg Rel Change: 0.883151% ± 0.095226%, Avg Cosine Sim: 0.999961 ± 0.000009\n",
      "Layer: model.layers.6.mlp.down_proj.weight, Avg Abs Change: 0.001634 ± 0.000061, Avg Rel Change: 0.886423% ± 0.062337%, Avg Cosine Sim: 0.999961 ± 0.000006\n",
      "Layer: model.layers.7.mlp.gate_proj.weight, Avg Abs Change: 0.001626 ± 0.000185, Avg Rel Change: 0.777353% ± 0.111991%, Avg Cosine Sim: 0.999969 ± 0.000009\n",
      "Layer: model.layers.7.mlp.up_proj.weight, Avg Abs Change: 0.001631 ± 0.000159, Avg Rel Change: 0.881971% ± 0.090826%, Avg Cosine Sim: 0.999961 ± 0.000008\n",
      "Layer: model.layers.7.mlp.down_proj.weight, Avg Abs Change: 0.001631 ± 0.000061, Avg Rel Change: 0.891098% ± 0.080968%, Avg Cosine Sim: 0.999960 ± 0.000009\n",
      "Layer: model.layers.8.mlp.gate_proj.weight, Avg Abs Change: 0.001651 ± 0.000190, Avg Rel Change: 0.793526% ± 0.120534%, Avg Cosine Sim: 0.999968 ± 0.000010\n",
      "Layer: model.layers.8.mlp.up_proj.weight, Avg Abs Change: 0.001664 ± 0.000164, Avg Rel Change: 0.891591% ± 0.092377%, Avg Cosine Sim: 0.999960 ± 0.000008\n",
      "Layer: model.layers.8.mlp.down_proj.weight, Avg Abs Change: 0.001668 ± 0.000064, Avg Rel Change: 0.905522% ± 0.093513%, Avg Cosine Sim: 0.999959 ± 0.000010\n",
      "Layer: model.layers.9.mlp.gate_proj.weight, Avg Abs Change: 0.001665 ± 0.000186, Avg Rel Change: 0.809717% ± 0.124526%, Avg Cosine Sim: 0.999967 ± 0.000010\n",
      "Layer: model.layers.9.mlp.up_proj.weight, Avg Abs Change: 0.001688 ± 0.000158, Avg Rel Change: 0.896490% ± 0.091025%, Avg Cosine Sim: 0.999960 ± 0.000008\n",
      "Layer: model.layers.9.mlp.down_proj.weight, Avg Abs Change: 0.001675 ± 0.000065, Avg Rel Change: 0.905141% ± 0.104721%, Avg Cosine Sim: 0.999959 ± 0.000011\n",
      "Layer: model.layers.10.mlp.gate_proj.weight, Avg Abs Change: 0.001644 ± 0.000188, Avg Rel Change: 0.808286% ± 0.131500%, Avg Cosine Sim: 0.999967 ± 0.000011\n",
      "Layer: model.layers.10.mlp.up_proj.weight, Avg Abs Change: 0.001641 ± 0.000161, Avg Rel Change: 0.868764% ± 0.094942%, Avg Cosine Sim: 0.999962 ± 0.000008\n",
      "Layer: model.layers.10.mlp.down_proj.weight, Avg Abs Change: 0.001642 ± 0.000070, Avg Rel Change: 0.883471% ± 0.088353%, Avg Cosine Sim: 0.999961 ± 0.000009\n",
      "Layer: model.layers.11.mlp.gate_proj.weight, Avg Abs Change: 0.001654 ± 0.000199, Avg Rel Change: 0.821460% ± 0.148815%, Avg Cosine Sim: 0.999966 ± 0.000013\n",
      "Layer: model.layers.11.mlp.up_proj.weight, Avg Abs Change: 0.001661 ± 0.000169, Avg Rel Change: 0.873864% ± 0.101620%, Avg Cosine Sim: 0.999961 ± 0.000009\n",
      "Layer: model.layers.11.mlp.down_proj.weight, Avg Abs Change: 0.001650 ± 0.000076, Avg Rel Change: 0.883536% ± 0.083870%, Avg Cosine Sim: 0.999961 ± 0.000008\n",
      "Layer: model.layers.12.mlp.gate_proj.weight, Avg Abs Change: 0.001639 ± 0.000197, Avg Rel Change: 0.820592% ± 0.141242%, Avg Cosine Sim: 0.999966 ± 0.000012\n",
      "Layer: model.layers.12.mlp.up_proj.weight, Avg Abs Change: 0.001640 ± 0.000164, Avg Rel Change: 0.860360% ± 0.096855%, Avg Cosine Sim: 0.999963 ± 0.000008\n",
      "Layer: model.layers.12.mlp.down_proj.weight, Avg Abs Change: 0.001650 ± 0.000071, Avg Rel Change: 0.884681% ± 0.102004%, Avg Cosine Sim: 0.999961 ± 0.000010\n",
      "Layer: model.layers.13.mlp.gate_proj.weight, Avg Abs Change: 0.001625 ± 0.000206, Avg Rel Change: 0.822851% ± 0.146548%, Avg Cosine Sim: 0.999965 ± 0.000013\n",
      "Layer: model.layers.13.mlp.up_proj.weight, Avg Abs Change: 0.001635 ± 0.000176, Avg Rel Change: 0.853141% ± 0.103501%, Avg Cosine Sim: 0.999963 ± 0.000009\n",
      "Layer: model.layers.13.mlp.down_proj.weight, Avg Abs Change: 0.001653 ± 0.000077, Avg Rel Change: 0.877113% ± 0.092371%, Avg Cosine Sim: 0.999961 ± 0.000009\n",
      "Layer: model.layers.14.mlp.gate_proj.weight, Avg Abs Change: 0.001621 ± 0.000220, Avg Rel Change: 0.822436% ± 0.160832%, Avg Cosine Sim: 0.999965 ± 0.000014\n",
      "Layer: model.layers.14.mlp.up_proj.weight, Avg Abs Change: 0.001642 ± 0.000185, Avg Rel Change: 0.847952% ± 0.110831%, Avg Cosine Sim: 0.999964 ± 0.000010\n",
      "Layer: model.layers.14.mlp.down_proj.weight, Avg Abs Change: 0.001659 ± 0.000085, Avg Rel Change: 0.870721% ± 0.079493%, Avg Cosine Sim: 0.999962 ± 0.000008\n",
      "Layer: model.layers.15.mlp.gate_proj.weight, Avg Abs Change: 0.001630 ± 0.000227, Avg Rel Change: 0.815502% ± 0.154645%, Avg Cosine Sim: 0.999966 ± 0.000013\n",
      "Layer: model.layers.15.mlp.up_proj.weight, Avg Abs Change: 0.001643 ± 0.000181, Avg Rel Change: 0.852588% ± 0.107778%, Avg Cosine Sim: 0.999963 ± 0.000010\n",
      "Layer: model.layers.15.mlp.down_proj.weight, Avg Abs Change: 0.001665 ± 0.000079, Avg Rel Change: 0.879226% ± 0.093933%, Avg Cosine Sim: 0.999961 ± 0.000009\n",
      "Layer: model.layers.16.mlp.gate_proj.weight, Avg Abs Change: 0.001641 ± 0.000236, Avg Rel Change: 0.811388% ± 0.158734%, Avg Cosine Sim: 0.999966 ± 0.000013\n",
      "Layer: model.layers.16.mlp.up_proj.weight, Avg Abs Change: 0.001648 ± 0.000179, Avg Rel Change: 0.858958% ± 0.108176%, Avg Cosine Sim: 0.999963 ± 0.000010\n",
      "Layer: model.layers.16.mlp.down_proj.weight, Avg Abs Change: 0.001672 ± 0.000081, Avg Rel Change: 0.886826% ± 0.094895%, Avg Cosine Sim: 0.999960 ± 0.000010\n",
      "Layer: model.layers.17.mlp.gate_proj.weight, Avg Abs Change: 0.001638 ± 0.000247, Avg Rel Change: 0.808460% ± 0.174942%, Avg Cosine Sim: 0.999966 ± 0.000015\n",
      "Layer: model.layers.17.mlp.up_proj.weight, Avg Abs Change: 0.001643 ± 0.000188, Avg Rel Change: 0.852247% ± 0.117745%, Avg Cosine Sim: 0.999963 ± 0.000011\n",
      "Layer: model.layers.17.mlp.down_proj.weight, Avg Abs Change: 0.001673 ± 0.000084, Avg Rel Change: 0.878529% ± 0.078517%, Avg Cosine Sim: 0.999961 ± 0.000008\n",
      "Layer: model.layers.18.mlp.gate_proj.weight, Avg Abs Change: 0.001623 ± 0.000261, Avg Rel Change: 0.799602% ± 0.175869%, Avg Cosine Sim: 0.999967 ± 0.000014\n",
      "Layer: model.layers.18.mlp.up_proj.weight, Avg Abs Change: 0.001637 ± 0.000192, Avg Rel Change: 0.848949% ± 0.119853%, Avg Cosine Sim: 0.999963 ± 0.000011\n",
      "Layer: model.layers.18.mlp.down_proj.weight, Avg Abs Change: 0.001672 ± 0.000084, Avg Rel Change: 0.877819% ± 0.064624%, Avg Cosine Sim: 0.999962 ± 0.000006\n",
      "Layer: model.layers.19.mlp.gate_proj.weight, Avg Abs Change: 0.001622 ± 0.000265, Avg Rel Change: 0.786691% ± 0.171914%, Avg Cosine Sim: 0.999968 ± 0.000014\n",
      "Layer: model.layers.19.mlp.up_proj.weight, Avg Abs Change: 0.001647 ± 0.000197, Avg Rel Change: 0.855254% ± 0.119663%, Avg Cosine Sim: 0.999963 ± 0.000011\n",
      "Layer: model.layers.19.mlp.down_proj.weight, Avg Abs Change: 0.001677 ± 0.000083, Avg Rel Change: 0.881870% ± 0.069027%, Avg Cosine Sim: 0.999961 ± 0.000007\n",
      "Layer: model.layers.20.mlp.gate_proj.weight, Avg Abs Change: 0.001626 ± 0.000263, Avg Rel Change: 0.776464% ± 0.160677%, Avg Cosine Sim: 0.999969 ± 0.000013\n",
      "Layer: model.layers.20.mlp.up_proj.weight, Avg Abs Change: 0.001644 ± 0.000194, Avg Rel Change: 0.860718% ± 0.113191%, Avg Cosine Sim: 0.999963 ± 0.000010\n",
      "Layer: model.layers.20.mlp.down_proj.weight, Avg Abs Change: 0.001672 ± 0.000079, Avg Rel Change: 0.887162% ± 0.091161%, Avg Cosine Sim: 0.999960 ± 0.000010\n",
      "Layer: model.layers.21.mlp.gate_proj.weight, Avg Abs Change: 0.001622 ± 0.000260, Avg Rel Change: 0.755600% ± 0.150492%, Avg Cosine Sim: 0.999971 ± 0.000012\n",
      "Layer: model.layers.21.mlp.up_proj.weight, Avg Abs Change: 0.001635 ± 0.000192, Avg Rel Change: 0.869696% ± 0.110917%, Avg Cosine Sim: 0.999962 ± 0.000010\n",
      "Layer: model.layers.21.mlp.down_proj.weight, Avg Abs Change: 0.001657 ± 0.000079, Avg Rel Change: 0.892869% ± 0.104963%, Avg Cosine Sim: 0.999960 ± 0.000012\n",
      "Layer: model.layers.22.mlp.gate_proj.weight, Avg Abs Change: 0.001624 ± 0.000250, Avg Rel Change: 0.754892% ± 0.140966%, Avg Cosine Sim: 0.999971 ± 0.000011\n",
      "Layer: model.layers.22.mlp.up_proj.weight, Avg Abs Change: 0.001631 ± 0.000191, Avg Rel Change: 0.865346% ± 0.108819%, Avg Cosine Sim: 0.999962 ± 0.000010\n",
      "Layer: model.layers.22.mlp.down_proj.weight, Avg Abs Change: 0.001653 ± 0.000077, Avg Rel Change: 0.888007% ± 0.107459%, Avg Cosine Sim: 0.999960 ± 0.000013\n",
      "Layer: model.layers.23.mlp.gate_proj.weight, Avg Abs Change: 0.001631 ± 0.000247, Avg Rel Change: 0.754127% ± 0.135596%, Avg Cosine Sim: 0.999971 ± 0.000010\n",
      "Layer: model.layers.23.mlp.up_proj.weight, Avg Abs Change: 0.001623 ± 0.000191, Avg Rel Change: 0.861269% ± 0.107730%, Avg Cosine Sim: 0.999963 ± 0.000010\n",
      "Layer: model.layers.23.mlp.down_proj.weight, Avg Abs Change: 0.001644 ± 0.000078, Avg Rel Change: 0.883698% ± 0.115499%, Avg Cosine Sim: 0.999961 ± 0.000014\n",
      "Layer: model.layers.24.mlp.gate_proj.weight, Avg Abs Change: 0.001632 ± 0.000249, Avg Rel Change: 0.747598% ± 0.134232%, Avg Cosine Sim: 0.999972 ± 0.000010\n",
      "Layer: model.layers.24.mlp.up_proj.weight, Avg Abs Change: 0.001619 ± 0.000191, Avg Rel Change: 0.860570% ± 0.107683%, Avg Cosine Sim: 0.999963 ± 0.000009\n",
      "Layer: model.layers.24.mlp.down_proj.weight, Avg Abs Change: 0.001640 ± 0.000082, Avg Rel Change: 0.883966% ± 0.125173%, Avg Cosine Sim: 0.999960 ± 0.000015\n",
      "Layer: model.layers.25.mlp.gate_proj.weight, Avg Abs Change: 0.001625 ± 0.000251, Avg Rel Change: 0.744515% ± 0.133059%, Avg Cosine Sim: 0.999972 ± 0.000010\n",
      "Layer: model.layers.25.mlp.up_proj.weight, Avg Abs Change: 0.001611 ± 0.000199, Avg Rel Change: 0.851790% ± 0.109979%, Avg Cosine Sim: 0.999963 ± 0.000010\n",
      "Layer: model.layers.25.mlp.down_proj.weight, Avg Abs Change: 0.001633 ± 0.000084, Avg Rel Change: 0.876005% ± 0.135806%, Avg Cosine Sim: 0.999961 ± 0.000018\n",
      "Layer: model.layers.26.mlp.gate_proj.weight, Avg Abs Change: 0.001618 ± 0.000248, Avg Rel Change: 0.745539% ± 0.133487%, Avg Cosine Sim: 0.999972 ± 0.000010\n",
      "Layer: model.layers.26.mlp.up_proj.weight, Avg Abs Change: 0.001608 ± 0.000202, Avg Rel Change: 0.841060% ± 0.111248%, Avg Cosine Sim: 0.999964 ± 0.000009\n",
      "Layer: model.layers.26.mlp.down_proj.weight, Avg Abs Change: 0.001632 ± 0.000083, Avg Rel Change: 0.865960% ± 0.138496%, Avg Cosine Sim: 0.999962 ± 0.000018\n",
      "Layer: model.layers.27.mlp.gate_proj.weight, Avg Abs Change: 0.001596 ± 0.000257, Avg Rel Change: 0.736097% ± 0.139832%, Avg Cosine Sim: 0.999972 ± 0.000010\n",
      "Layer: model.layers.27.mlp.up_proj.weight, Avg Abs Change: 0.001596 ± 0.000213, Avg Rel Change: 0.829067% ± 0.117959%, Avg Cosine Sim: 0.999965 ± 0.000010\n",
      "Layer: model.layers.27.mlp.down_proj.weight, Avg Abs Change: 0.001620 ± 0.000088, Avg Rel Change: 0.851786% ± 0.132289%, Avg Cosine Sim: 0.999963 ± 0.000016\n",
      "Layer: model.layers.28.mlp.gate_proj.weight, Avg Abs Change: 0.001612 ± 0.000250, Avg Rel Change: 0.750818% ± 0.145264%, Avg Cosine Sim: 0.999971 ± 0.000011\n",
      "Layer: model.layers.28.mlp.up_proj.weight, Avg Abs Change: 0.001625 ± 0.000223, Avg Rel Change: 0.833291% ± 0.124161%, Avg Cosine Sim: 0.999965 ± 0.000011\n",
      "Layer: model.layers.28.mlp.down_proj.weight, Avg Abs Change: 0.001641 ± 0.000094, Avg Rel Change: 0.852071% ± 0.126859%, Avg Cosine Sim: 0.999963 ± 0.000016\n",
      "Layer: model.layers.29.mlp.gate_proj.weight, Avg Abs Change: 0.001646 ± 0.000244, Avg Rel Change: 0.773449% ± 0.148461%, Avg Cosine Sim: 0.999969 ± 0.000012\n",
      "Layer: model.layers.29.mlp.up_proj.weight, Avg Abs Change: 0.001656 ± 0.000232, Avg Rel Change: 0.839759% ± 0.126397%, Avg Cosine Sim: 0.999964 ± 0.000011\n",
      "Layer: model.layers.29.mlp.down_proj.weight, Avg Abs Change: 0.001674 ± 0.000096, Avg Rel Change: 0.864031% ± 0.118677%, Avg Cosine Sim: 0.999962 ± 0.000015\n",
      "Layer: model.layers.30.mlp.gate_proj.weight, Avg Abs Change: 0.001677 ± 0.000235, Avg Rel Change: 0.788017% ± 0.149385%, Avg Cosine Sim: 0.999968 ± 0.000012\n",
      "Layer: model.layers.30.mlp.up_proj.weight, Avg Abs Change: 0.001684 ± 0.000243, Avg Rel Change: 0.843411% ± 0.136440%, Avg Cosine Sim: 0.999964 ± 0.000012\n",
      "Layer: model.layers.30.mlp.down_proj.weight, Avg Abs Change: 0.001688 ± 0.000108, Avg Rel Change: 0.866481% ± 0.095158%, Avg Cosine Sim: 0.999962 ± 0.000011\n",
      "Layer: model.layers.31.mlp.gate_proj.weight, Avg Abs Change: 0.001622 ± 0.000232, Avg Rel Change: 0.733685% ± 0.148913%, Avg Cosine Sim: 0.999972 ± 0.000011\n",
      "Layer: model.layers.31.mlp.up_proj.weight, Avg Abs Change: 0.001619 ± 0.000265, Avg Rel Change: 0.794393% ± 0.145236%, Avg Cosine Sim: 0.999968 ± 0.000012\n",
      "Layer: model.layers.31.mlp.down_proj.weight, Avg Abs Change: 0.001594 ± 0.000125, Avg Rel Change: 0.851221% ± 0.073493%, Avg Cosine Sim: 0.999964 ± 0.000007\n",
      "\n",
      "=== Overall Statistics Across All Layers ===\n",
      "Overall Avg Absolute Change: 0.001625 ± 0.000041\n",
      "Overall Avg Relative Change: 0.833093% ± 0.047845%\n",
      "Overall Avg Cosine Similarity: 0.999965 ± 0.000004\n"
     ]
    }
   ],
   "source": [
    "# Store per-layer results\n",
    "layer_differences = {}\n",
    "\n",
    "for name in mlp_layer_names:\n",
    "    W1 = mlp_layers_pt[name].detach().cpu().to(torch.float32)  # Weights before DPO\n",
    "    W2 = mlp_layers_dpo[name].detach().cpu().to(torch.float32)  # Weights after DPO\n",
    "    \n",
    "    W1 = W1.view(-1, 4096)\n",
    "    W2 = W2.view(-1, 4096)\n",
    "\n",
    "    num_vectors = W1.shape[0]  # Number of vectors (4*d)\n",
    "    vector_size = W1.shape[1]  # Size of each vector (d)\n",
    "    \n",
    "    # print(W2.shape)\n",
    "\n",
    "    # Compute per-vector absolute change (L2 norm)\n",
    "    abs_change = torch.norm(W2 - W1, dim=1)  \n",
    "    \n",
    "    # Compute per-vector relative change\n",
    "    norm_W1 = torch.norm(W1, dim=1) + 1e-8 \n",
    "    rel_change = abs_change / norm_W1  \n",
    "    \n",
    "    # Compute per-vector cosine similarity\n",
    "    W1_normed = W1 / norm_W1.unsqueeze(1)  \n",
    "    W2_normed = W2 / (torch.norm(W2, dim=1, keepdim=True) + 1e-8)  \n",
    "    cosine_sim = (W1_normed * W2_normed).sum(dim=1)  \n",
    "\n",
    "    # Compute averages and standard deviations across all vectors in the layer\n",
    "    avg_abs_change = abs_change.mean().item()\n",
    "    std_abs_change = abs_change.std().item()\n",
    "\n",
    "    avg_rel_change = rel_change.mean().item()\n",
    "    std_rel_change = rel_change.std().item()\n",
    "\n",
    "    avg_cosine_sim = cosine_sim.mean().item()\n",
    "    std_cosine_sim = cosine_sim.std().item()\n",
    "\n",
    "    # Store results\n",
    "    layer_differences[name] = {\n",
    "        \"avg_absolute_change\": avg_abs_change,\n",
    "        \"std_absolute_change\": std_abs_change,\n",
    "        \"avg_relative_change\": avg_rel_change,\n",
    "        \"std_relative_change\": std_rel_change,\n",
    "        \"avg_cosine_similarity\": avg_cosine_sim,\n",
    "        \"std_cosine_similarity\": std_cosine_sim\n",
    "    }\n",
    "\n",
    "# Compute overall mean and standard deviation across all layers\n",
    "total_layers = len(layer_differences)\n",
    "\n",
    "all_abs_changes = torch.tensor([layer[\"avg_absolute_change\"] for layer in layer_differences.values()])\n",
    "all_rel_changes = torch.tensor([layer[\"avg_relative_change\"] for layer in layer_differences.values()])\n",
    "all_cosine_sims = torch.tensor([layer[\"avg_cosine_similarity\"] for layer in layer_differences.values()])\n",
    "\n",
    "overall_avg_abs_change = all_abs_changes.mean().item()\n",
    "overall_std_abs_change = all_abs_changes.std().item()\n",
    "\n",
    "overall_avg_rel_change = all_rel_changes.mean().item()\n",
    "overall_std_rel_change = all_rel_changes.std().item()\n",
    "\n",
    "overall_avg_cosine_sim = all_cosine_sims.mean().item()\n",
    "overall_std_cosine_sim = all_cosine_sims.std().item()\n",
    "\n",
    "# Print per-layer results\n",
    "for layer, changes in layer_differences.items():\n",
    "    print(f\"Layer: {layer}, Avg Abs Change: {changes['avg_absolute_change']:.6f} ± {changes['std_absolute_change']:.6f}, \"\n",
    "          f\"Avg Rel Change: {changes['avg_relative_change']:.6%} ± {changes['std_relative_change']:.6%}, \"\n",
    "          f\"Avg Cosine Sim: {changes['avg_cosine_similarity']:.6f} ± {changes['std_cosine_similarity']:.6f}\")\n",
    "\n",
    "# Print overall averages and standard deviations\n",
    "print(\"\\n=== Overall Statistics Across All Layers ===\")\n",
    "print(f\"Overall Avg Absolute Change: {overall_avg_abs_change:.6f} ± {overall_std_abs_change:.6f}\")\n",
    "print(f\"Overall Avg Relative Change: {overall_avg_rel_change:.6%} ± {overall_std_rel_change:.6%}\")\n",
    "print(f\"Overall Avg Cosine Similarity: {overall_avg_cosine_sim:.6f} ± {overall_std_cosine_sim:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
