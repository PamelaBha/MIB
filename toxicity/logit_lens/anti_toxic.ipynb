{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b72fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a926b9b",
   "metadata": {},
   "source": [
    "#### GPT2-medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04a80b",
   "metadata": {},
   "source": [
    "##### Use probe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a03045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/gpt2_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152b802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aea1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98304, 1024])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.transformer.wte.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.transformer.h[layer_idx].mlp.c_proj.weight\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4120f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.transformer.ln_f\n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = einsum(\"vocab d_model, d_model -> vocab\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b212567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 10, index 1882\n",
      "['ardo', ' maniac', '@#', 'ueless', ' counterfe', ' thug', ' arrog', 'asta', ' disregard', ' fucking']\n",
      "Value vec: Layer 7, index 3094\n",
      "[' wasting', 'urance', 'fee', ' waste', 'ternity', ' gratification', ' ank', 'closure', 'wa', ' fuss']\n",
      "Value vec: Layer 11, index 1307\n",
      "[' damn', ' darn', ' kidding', '!).', ' freaking', ' booze', ' piss', '!.', ' damned', '!)']\n",
      "Value vec: Layer 15, index 301\n",
      "[' harmful', ' Worse', ' unfavorable', ' disturbing', ' unpleasant', ' detrimental', ' undesirable', ' restrictive', ' problematic', ' inconsistent']\n",
      "Value vec: Layer 6, index 2728\n",
      "['naires', 'acci', 'emed', 'aple', 'assis', ' platoon', 'amn', 'phe', 'oux', 'ays']\n",
      "Value vec: Layer 5, index 528\n",
      "['veyard', ' Aven', 'ija', 'bars', 'enburg', 'phant', 'aiden', 'andro', 'ppo', ' Britann']\n",
      "Value vec: Layer 4, index 2123\n",
      "['ipop', 'spin', 'umi', 'office', 'ULE', ' tables', '765', 'ji', 'achi', ' Rosenberg']\n",
      "Value vec: Layer 3, index 4021\n",
      "[' Huck', 'ubs', 'INESS', 'doms', 'abouts', 'away', 'estone', 'LOAD', ' Hebdo', 'pots']\n",
      "Value vec: Layer 20, index 3859\n",
      "['ombat', ' Glas', ' Antar', 'urized', ' Purg', 'eri', ' BDS', ' predic', ' pilgr', 'oS']\n",
      "Value vec: Layer 4, index 2136\n",
      "[' indeed', ' reven', ' Victorian', ' PCB', ' dist', ' barring', 'IRD', 'monkey', ' bars', ' Sawyer']\n",
      "Value vec: Layer 7, index 3836\n",
      "[' IMAGES', ' Carey', ' Feet', ' Kardash', ' hearts', ' Budd', ' EQU', ' Bosh', ' arous', ' Harding']\n",
      "Value vec: Layer 6, index 780\n",
      "[' nodd', 'acco', 'orno', 'neum', 'azo', ' inhab', 'rint', ' resid', 'alach', ' extermination']\n",
      "Value vec: Layer 16, index 2529\n",
      "['Cam', ' successor', ' fluent', 'complete', ' Seaf', ' Cam', 'faced', ' inher', ' Vern', ' Expedition']\n",
      "Value vec: Layer 10, index 183\n",
      "[' clocks', 'cker', ' discharged', ' estates', ' afterlife', 'gel', 'Rail', ' bott', ' hanged', ' disembark']\n",
      "Value vec: Layer 5, index 4090\n",
      "['icipated', ' disqual', ' hemorrh', ' unhealthy', ' asymm', ' cens', ' freeze', ' throttle', ' censor', 'farious']\n",
      "Value vec: Layer 16, index 1354\n",
      "['VEN', ' throat', 'ufact', ' corner', ' HUN', ' nost', 'parser', 'ific', ' lungs', 'ulin']\n",
      "Value vec: Layer 20, index 516\n",
      "[' dividends', ' morbid', ' proverbial', ' stringent', 'roying', ' moniker', ' Pip', ' bount', ' propos', ' occupying']\n",
      "Value vec: Layer 10, index 4026\n",
      "['Reloaded', ' Franch', ' RIS', ' Option', '���', \"'';\", ' UNIVERS', ' Bastard', ' intrins', ' Bie']\n",
      "Value vec: Layer 13, index 1273\n",
      "[' Duck', ' Rock', ' ROCK', ' peel', 'cat', ' half', 'rock', ' Context', ' Tens', ' lick']\n",
      "Value vec: Layer 17, index 448\n",
      "['pload', 'emis', 'glomer', 'trap', 'Anth', 'gen', 'por', 'cious', 'Pak', 'yt']\n",
      "Value vec: Layer 4, index 4037\n",
      "['habi', ' conspir', 'hetti', 'Benz', ' fixation', ' taxp', 'glomer', ' Solitaire', 'aband', ' Cologne']\n",
      "Value vec: Layer 8, index 3831\n",
      "[' childbirth', '�', ' parenting', 'STD', ' Editorial', '~~~~', ' Lup', ' crem', ' Breast', ' recommended']\n",
      "Value vec: Layer 2, index 1088\n",
      "[' GER', ' EXP', ' itiner', 'pty', ' nuts', ' idiots', 'anche', ' NE', ' POV', ' royalty']\n",
      "Value vec: Layer 11, index 658\n",
      "[' Camer', ' RAND', ' ASS', ' Mehran', ' SHA', ' PAN', ' GOLD', ' Dodge', ' RAD', ' STL']\n",
      "Value vec: Layer 19, index 591\n",
      "[' uphill', ' cleared', 'ool', 'urst', ' backwards', 'Completed', ' squared', 'Translation', 'inational', 'alion']\n",
      "Value vec: Layer 14, index 337\n",
      "['irin', ' Godd', 'ELF', 'raved', ' gad', 'hou', 'oup', 'sup', 'hots', 'wagon']\n",
      "Value vec: Layer 22, index 33\n",
      "[' off', ' away', ' OFF', 'off', ' Off', ' Away', ' out', 'Off', 'offs', 'OFF']\n",
      "Value vec: Layer 16, index 2576\n",
      "['Write', 'Apply', 'Submit', ' seek', 'apply', ' Submit', 'write', 'submit', 'fill', ' WRITE']\n",
      "Value vec: Layer 7, index 3331\n",
      "[' resil', 'cca', 'oca', 'uana', 'etheus', '��', 'ould', 'ocket', 'ffee', 'orously']\n",
      "Value vec: Layer 6, index 898\n",
      "[' anonym', ' transl', ' Unicode', ' bilingual', ' scans', ' authenticated', ' copy', ' surrog', ' Lia', ' accompany']\n",
      "Value vec: Layer 11, index 3869\n",
      "[' opposing', 'ervatives', ' dispute', ' opponents', ' bullies', ' opposition', 'ophob', ' bullshit', ' defiance', ' disple']\n",
      "Value vec: Layer 13, index 1043\n",
      "[' gag', ' uncle', ' dope', 'endum', ' enjoying', ' Zombie', ' Disciple', ' guiName', '======', ' puppet']\n",
      "Value vec: Layer 12, index 300\n",
      "[' Addiction', 'ulner', ' eas', ' addict', ' Bris', 'ggles', ' grants', ' Categories', ' combos', ' dstg']\n",
      "Value vec: Layer 16, index 1798\n",
      "['MpServer', 'accompan', 'asper', 'estate', 'ridges', 'krit', 'hal', 'animate', 'Engineers', 'agents']\n",
      "Value vec: Layer 18, index 1369\n",
      "[' bogus', ' bullshit', ' illegally', ' dishonest', ' abusive', ' unfairly', ' disingen', ' improperly', ' corrupt', ' falsely']\n",
      "Value vec: Layer 12, index 982\n",
      "['icious', 'iously', ' darn', ' mischief', ' excuses', ' absurdity', 'acebook', ' stupidity', 'edly', 'farious']\n",
      "Value vec: Layer 19, index 2520\n",
      "['Down', ' Wast', 'eny', 'Words', 'orus', 'HI', 'riad', 'eat', 'omorph', 'advertising']\n",
      "Value vec: Layer 6, index 1256\n",
      "[' unden', ' reservations', ' plag', ' dehuman', ' abandonment', ' bere', ' disag', ' ceases', ' devoid', ' clients']\n",
      "Value vec: Layer 20, index 988\n",
      "[' damaging', ' misuse', ' ruin', ' aggravated', ' Worse', ' detriment', ' undesirable', ' detrimental', ' inadequ', ' tainted']\n",
      "Value vec: Layer 11, index 2276\n",
      "[' extinguished', 'andom', ' blunt', 'usterity', ' irresistible', 'oldown', 'ttle', 'SPONSORED', 'estyles', 'atural']\n",
      "Value vec: Layer 15, index 509\n",
      "[' taxp', 'thinkable', ' thous', 'NYSE', ' municip', 'putable', ' stru', ' manag', 'idated', '中']\n",
      "Value vec: Layer 2, index 976\n",
      "[' moan', ' fucked', ' sucked', ' pissed', ' moaning', ' nailed', ' basically', ' shit', ' shitty', ' blackout']\n",
      "Value vec: Layer 15, index 1874\n",
      "[' Incarn', ' Subtle', ' Prepar', ' Desire', ' Serpent', ' Generation', ' Rapt', ' Sim', ' Reflex', ' Associates']\n",
      "Value vec: Layer 8, index 2050\n",
      "[' dehuman', ' hipp', ' guilty', ' colonization', 'cale', ' nationalism', ' euphem', ' unborn', ' conservatism', ' WAS']\n",
      "Value vec: Layer 5, index 3051\n",
      "['bred', ' surpr', ' tiss', 'entious', 'achi', 'olesterol', 'abby', \"'';\", ' paraph', 'bare']\n",
      "Value vec: Layer 16, index 2190\n",
      "[' wrists', ' Bought', 'olics', ' simulac', 'avorite', ' Webs', 'blogspot', ' grief', 'cffff', '////////////////']\n",
      "Value vec: Layer 10, index 1380\n",
      "[' overc', ' tired', ' fuss', ' sadd', ' tearing', 'loo', 'atform', ' messed', ' awa', ' tricked']\n",
      "Value vec: Layer 19, index 3093\n",
      "[' popcorn', ' Rebell', ' Rebellion', ' pockets', ' taxp', ' snipp', ' Hamb', ' prosec', ' contests', ' horizont']\n",
      "Value vec: Layer 2, index 649\n",
      "[' energy', ' overdose', ' radiation', ' irrad', ' dose', ' curfew', ' contagious', ' asleep', ' Inferno', ' aversion']\n",
      "Value vec: Layer 10, index 2929\n",
      "[' vain', ' fails', ' fail', ' useless', ' inappropriately', ' failed', ' unnecessarily', ' Failed', ' unsu', ' incompatible']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 50\n",
    "norm = model.transformer.ln_f\n",
    "\n",
    "dot_prods = einsum(\"value_vecs d_model, d_model -> value_vecs\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k, largest=False).indices\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(-1*value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c5af2",
   "metadata": {},
   "source": [
    "##### Use extracted cossim from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ebc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/kebl6672/dpo-toxic-general/toxicity/gpt2_all_neuron_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bc60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af9554bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98304, 1024])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.transformer.wte.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.transformer.h[layer_idx].mlp.c_proj.weight\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c83ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neurons = df.nsmallest(30, 'pt_cossim')\n",
    "\n",
    "top_layer_indices = top_neurons['layer_idx'].values \n",
    "top_neuron_indices = top_neurons['neuron_idx'].values \n",
    "\n",
    "top_neuron_tuples = list(zip(top_layer_indices, top_neuron_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da07d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.transformer.ln_f\n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = einsum(\"vocab d_model, d_model -> vocab\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83faf4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 10, index 1882\n",
      "['ardo', ' maniac', '@#', 'ueless', ' counterfe', ' thug', ' arrog', 'asta', ' disregard', ' fucking']\n",
      "Value vec: Layer 11, index 1307\n",
      "[' damn', ' darn', ' kidding', '!).', ' freaking', ' booze', ' piss', '!.', ' damned', '!)']\n",
      "Value vec: Layer 7, index 3094\n",
      "[' wasting', 'urance', 'fee', ' waste', 'ternity', ' gratification', ' ank', 'closure', 'wa', ' fuss']\n",
      "Value vec: Layer 4, index 2123\n",
      "['ipop', 'spin', 'umi', 'office', 'ULE', ' tables', '765', 'ji', 'achi', ' Rosenberg']\n",
      "Value vec: Layer 15, index 301\n",
      "[' harmful', ' Worse', ' unfavorable', ' disturbing', ' unpleasant', ' detrimental', ' undesirable', ' restrictive', ' problematic', ' inconsistent']\n",
      "Value vec: Layer 3, index 4021\n",
      "[' Huck', 'ubs', 'INESS', 'doms', 'abouts', 'away', 'estone', 'LOAD', ' Hebdo', 'pots']\n",
      "Value vec: Layer 6, index 2728\n",
      "['naires', 'acci', 'emed', 'aple', 'assis', ' platoon', 'amn', 'phe', 'oux', 'ays']\n",
      "Value vec: Layer 5, index 528\n",
      "['veyard', ' Aven', 'ija', 'bars', 'enburg', 'phant', 'aiden', 'andro', 'ppo', ' Britann']\n",
      "Value vec: Layer 16, index 2529\n",
      "['Cam', ' successor', ' fluent', 'complete', ' Seaf', ' Cam', 'faced', ' inher', ' Vern', ' Expedition']\n",
      "Value vec: Layer 16, index 1798\n",
      "['MpServer', 'accompan', 'asper', 'estate', 'ridges', 'krit', 'hal', 'animate', 'Engineers', 'agents']\n",
      "Value vec: Layer 12, index 982\n",
      "['icious', 'iously', ' darn', ' mischief', ' excuses', ' absurdity', 'acebook', ' stupidity', 'edly', 'farious']\n",
      "Value vec: Layer 22, index 33\n",
      "[' off', ' away', ' OFF', 'off', ' Off', ' Away', ' out', 'Off', 'offs', 'OFF']\n",
      "Value vec: Layer 17, index 448\n",
      "['pload', 'emis', 'glomer', 'trap', 'Anth', 'gen', 'por', 'cious', 'Pak', 'yt']\n",
      "Value vec: Layer 7, index 3836\n",
      "[' IMAGES', ' Carey', ' Feet', ' Kardash', ' hearts', ' Budd', ' EQU', ' Bosh', ' arous', ' Harding']\n",
      "Value vec: Layer 20, index 516\n",
      "[' dividends', ' morbid', ' proverbial', ' stringent', 'roying', ' moniker', ' Pip', ' bount', ' propos', ' occupying']\n",
      "Value vec: Layer 16, index 1354\n",
      "['VEN', ' throat', 'ufact', ' corner', ' HUN', ' nost', 'parser', 'ific', ' lungs', 'ulin']\n",
      "Value vec: Layer 6, index 780\n",
      "[' nodd', 'acco', 'orno', 'neum', 'azo', ' inhab', 'rint', ' resid', 'alach', ' extermination']\n",
      "Value vec: Layer 5, index 4090\n",
      "['icipated', ' disqual', ' hemorrh', ' unhealthy', ' asymm', ' cens', ' freeze', ' throttle', ' censor', 'farious']\n",
      "Value vec: Layer 4, index 2136\n",
      "[' indeed', ' reven', ' Victorian', ' PCB', ' dist', ' barring', 'IRD', 'monkey', ' bars', ' Sawyer']\n",
      "Value vec: Layer 18, index 1369\n",
      "[' bogus', ' bullshit', ' illegally', ' dishonest', ' abusive', ' unfairly', ' disingen', ' improperly', ' corrupt', ' falsely']\n",
      "Value vec: Layer 16, index 3340\n",
      "[' bead', 'inventoryQuantity', 'leck', ' weighed', 'xious', ' juice', ' sweets', ' boy', ' ware', 'ibaba']\n",
      "Value vec: Layer 5, index 3051\n",
      "['bred', ' surpr', ' tiss', 'entious', 'achi', 'olesterol', 'abby', \"'';\", ' paraph', 'bare']\n",
      "Value vec: Layer 6, index 1256\n",
      "[' unden', ' reservations', ' plag', ' dehuman', ' abandonment', ' bere', ' disag', ' ceases', ' devoid', ' clients']\n",
      "Value vec: Layer 17, index 2038\n",
      "['bec', '=#', 'IAS', ' careers', 'Semitism', ' maternity', 'vs', ' friendships', 'esc', 'amental']\n",
      "Value vec: Layer 23, index 1274\n",
      "['psons', 'cled', 'alty', ' BLM', ' Mans', 'raid', ' Shant', 'rg', '═', 'bes']\n",
      "Value vec: Layer 14, index 337\n",
      "['irin', ' Godd', 'ELF', 'raved', ' gad', 'hou', 'oup', 'sup', 'hots', 'wagon']\n",
      "Value vec: Layer 6, index 898\n",
      "[' anonym', ' transl', ' Unicode', ' bilingual', ' scans', ' authenticated', ' copy', ' surrog', ' Lia', ' accompany']\n",
      "Value vec: Layer 20, index 3859\n",
      "['ombat', ' Glas', ' Antar', 'urized', ' Purg', 'eri', ' BDS', ' predic', ' pilgr', 'oS']\n",
      "Value vec: Layer 20, index 988\n",
      "[' damaging', ' misuse', ' ruin', ' aggravated', ' Worse', ' detriment', ' undesirable', ' detrimental', ' inadequ', ' tainted']\n",
      "Value vec: Layer 2, index 3078\n",
      "['Tar', 'Jar', 'CAN', 'Sham', 'Cath', 'Ger', 'Gar', 'hex', 'DER', 'Cart']\n"
     ]
    }
   ],
   "source": [
    "for (layer_idx, vec_idx) in top_neuron_tuples:\n",
    "    print(f\"Value vec: Layer {layer_idx}, index {vec_idx}\")\n",
    "    print(unembed_to_text(-1*value_vectors[layer_idx*4096+vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dfc0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "### Cossim between probe and toxic embedding\n",
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/gpt2_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4854d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "toxic_embed = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/gpt2_toxic_embed.pt\")\n",
    "print(toxic_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fbe0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between toxic_embed and toxic_probe: 0.2406\n"
     ]
    }
   ],
   "source": [
    "toxic_embed_norm = F.normalize(toxic_embed, dim=0)\n",
    "toxic_probe_norm = F.normalize(toxic_probe, dim=0)\n",
    "\n",
    "cossim = torch.dot(toxic_embed_norm, toxic_probe_norm)\n",
    "\n",
    "print(f\"Cosine similarity between toxic_embed and toxic_probe: {cossim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a395c",
   "metadata": {},
   "source": [
    "#### Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda5666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/llama3_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6195097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1255b8ffb12f4d2fadd3bafd08c719b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22bb926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5d8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcd42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 4, index 15020\n",
      "['azines', 'allery', 'ektor', 'rese', '/Dk', 'öl', ' Swinger', 'auga', 'eteria', 'hangi']\n",
      "Value vec: Layer 25, index 14671\n",
      "[' fuck', ' fucked', ' fucking', ' Fuck', 'Fuck', 'fuck', ' shit', ' FUCK', ' fucks', ' Fucking']\n",
      "Value vec: Layer 15, index 11954\n",
      "['tons', '/sbin', ' Williamson', '�', '절', '腕', 'cak', ' Jacqu', ' happening', 'краї']\n",
      "Value vec: Layer 23, index 5904\n",
      "[' statutes', ' statute', '游', ' Stat', 'oba', ' bust', ' verz', ' invalid', ' Colbert', 'uffy']\n",
      "Value vec: Layer 7, index 9882\n",
      "[' cow', 'ќ', 'wik', 'cratch', 'ewis', 'Cow', 'чи', ' unm', 'lectron', 'urement']\n",
      "Value vec: Layer 8, index 7154\n",
      "['oppable', 'acket', 'urname', 'mention', 'idata', 'anoia', 'reich', '글', 'scriptId', 'št']\n",
      "Value vec: Layer 23, index 2310\n",
      "['apesh', '이버', ' Hum', 'Hum', 'blr', ' tím', 'urtles', 'raya', 'пор', 'ensch']\n",
      "Value vec: Layer 20, index 8952\n",
      "['izi', 'zi', ' Pent', ' vaz', ' Clo', 'igar', 'edia', ' Zi', 'burgh', 'ubar']\n",
      "Value vec: Layer 2, index 12350\n",
      "['undry', '_pb', 'ثال', 'obj', 'äll', 'pon', '\\\\Carbon', 'duino', ' Υπο', '586']\n",
      "Value vec: Layer 3, index 15062\n",
      "[\"_('\", '_CT', '_ENSURE', 'illin', '_PRI', 'ides', '碎', '(nullable', '-et', 'Ћ']\n",
      "Value vec: Layer 22, index 8260\n",
      "['tm', 'uren', 'emple', ' Released', 'ulkan', 'mí', 'vos', 'meth', 'aux', 'ombie']\n",
      "Value vec: Layer 2, index 14142\n",
      "['lew', 'linger', 'readcr', ' Cyr', 'xFFFFFF', 'chied', ' mặt', 'ses', 'JKLM', 'ko']\n",
      "Value vec: Layer 14, index 15898\n",
      "[' carry', ' accompanies', ' accompany', ' carried', 'afort', ' handy', 'desk', '备', 'esa', ' travellers']\n",
      "Value vec: Layer 4, index 5971\n",
      "[' Kür', 'adlo', ' GetLastError', 'touch', 'ishly', '式', 'tober', 'TZ', 'orent', '907']\n",
      "Value vec: Layer 5, index 12532\n",
      "['etz', 'inski', '/MPL', 'Ε', 'uo', 'рава', ' Mutual', 'カー', 'arry', 'et']\n",
      "Value vec: Layer 7, index 10127\n",
      "['ailable', 'bens', 'iento', ' ><?', '弘', 'حل', '__$', 'eyen', 'enal', 'OOSE']\n",
      "Value vec: Layer 25, index 15326\n",
      "[' fe', ' Giles', 'antine', 'ster', '影', ' paran', ' Zac', 'lique', 'aster', 'pare']\n",
      "Value vec: Layer 0, index 16197\n",
      "[' 春', 'erdem', 'kil', 'atern', 'ække', 'elligent', 'ціональ', ' Шев', 'aginator', 'ăr']\n",
      "Value vec: Layer 14, index 4997\n",
      "[' sucks', ' suck', 'adla', ' awesome', 'BackPressed', 'teri', 'awesome', 'assage', '-awesome', 'ATO']\n",
      "Value vec: Layer 2, index 2682\n",
      "[' Lam', '/ng', ' passive', ' Nose', ' ep', 'aches', ' arch', ' Fauc', 'chner', 'του']\n",
      "Value vec: Layer 15, index 4146\n",
      "['kar', 'ORA', ' Directory', 'ora', ' nearest', 'urgent', 'alon', 'pak', 'jas', ' ew']\n",
      "Value vec: Layer 26, index 2983\n",
      "[' dest', ' Dest', 'ayer', 'ly', 'dest', ' fo', ' mult', 'Dest', 'enstein', '.internet']\n",
      "Value vec: Layer 22, index 13839\n",
      "['oons', 'onto', 'agna', 'les', 'ss', 'rani', 'oes', 'καν', 'idas', 'uments']\n",
      "Value vec: Layer 24, index 10951\n",
      "[' peer', ' cav', 'ey', ' tor', 'eren', '/to', 'TOR', 'CLR', ' Micro', 'arr']\n",
      "Value vec: Layer 12, index 3059\n",
      "['anlı', 'soft', '.soft', 'dorf', ' Greenwood', '刀', ' Operational', 'tec', ' slogan', 'ουσ']\n",
      "Value vec: Layer 13, index 12425\n",
      "['arine', 'FUNCTION', ' Dise', 'arton', 'FileVersion', '/apt', 'rego', 'arth', 'oids', ' функци']\n",
      "Value vec: Layer 3, index 10476\n",
      "['uhe', ' Arms', '.untracked', 'arms', 'ohl', 'ukt', ' drills', 'itag', '625', '�']\n",
      "Value vec: Layer 17, index 15351\n",
      "['urdy', 'APON', ' Gord', '132', 'HAV', '_WR', 'wrap', 'ango', ' prostit', 'ecure']\n",
      "Value vec: Layer 1, index 6037\n",
      "['igor', 'plorer', 'esch', 'uki', 'iliate', 'etter', 'üc', 'uka', ' zoekt', 'collapsed']\n",
      "Value vec: Layer 18, index 14425\n",
      "['mods', 'edin', 'ptrdiff', '846', 'çon', 'Saga', 'orge', ' отв', 'ardin', 'افق']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 30\n",
    "norm = model.model.norm  \n",
    "\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k, largest=False).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // (4096*4)}, index {vec_idx % (4096*4)}\")\n",
    "    print(unembed_to_text(-1*value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5e7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "### Cossim between probe and toxic embedding\n",
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/llama3_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2ff057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "toxic_embed = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/llama3_toxic_embed.pt\")\n",
    "print(toxic_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf16d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between toxic_embed and toxic_probe: 0.0787\n"
     ]
    }
   ],
   "source": [
    "toxic_embed_norm = F.normalize(toxic_embed, dim=0)\n",
    "toxic_probe_norm = F.normalize(toxic_probe, dim=0)\n",
    "\n",
    "cossim = torch.dot(toxic_embed_norm, toxic_probe_norm)\n",
    "\n",
    "print(f\"Cosine similarity between toxic_embed and toxic_probe: {cossim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f218a5",
   "metadata": {},
   "source": [
    "#### Gemma-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7760fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/kebl6672/dpo-toxic-general/toxicity/gemma2_all_neuron_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f42b484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9321e8cfb3e4417a8d2846e6f8eeca7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8969f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([239616, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e00d85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neurons = df.nsmallest(30, 'pt_cosine_similarity')\n",
    "\n",
    "top_layer_indices = top_neurons['layer_idx'].values \n",
    "top_neuron_indices = top_neurons['neuron_idx'].values \n",
    "\n",
    "top_neuron_tuples = list(zip(top_layer_indices, top_neuron_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dce896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54377428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 14, index 7822\n",
      "[' fucking', 'fucking', ' goddamn', ' Fucking', ' FUCKING', ' shit', 'Fucking', 'Fuck', ' damn', ' fuck']\n",
      "Value vec: Layer 6, index 7099\n",
      "[' fucking', 'fucking', 'fuck', 'Fucking', 'fucker', ' Fucking', 'FUCK', ' FUCKING', ' piss', 'Fuck']\n",
      "Value vec: Layer 15, index 3348\n",
      "['SBATCH', ' متعلقه', ' handel', 'StoryboardSegue', 'HasAnnotation', 'TestBed', ' krav', ' thick', '期刊论文', 'WriteBarrier']\n",
      "Value vec: Layer 24, index 6536\n",
      "[' historical', ' special', ' traditional', ' regular', ' individual', ' personal', ' exotic', ' massive', ' historic', ' professional']\n",
      "Value vec: Layer 6, index 3812\n",
      "['yntaxException', ' pleins', ' people', 'sendStatus', ' AttributeSet', 'epy', 'ContentLoaded', ' peoples', ' adil', 'asantry']\n",
      "Value vec: Layer 0, index 509\n",
      "[' téléchargez', ' GenerationType', ' volontaire', ' nadzieję', 'rowser', 'agd', 'Bilan', ' réaliste', 'SizeMode', ' parkir']\n",
      "Value vec: Layer 10, index 1114\n",
      "['nonatomic', 'PasswordField', '\"?>', 'WriteAttribute', 'Tikang', ' snippetHide', ' Weh', 'Safe', 'rootScope', 'като']\n",
      "Value vec: Layer 0, index 3064\n",
      "[' indulged', 'ety', '\\u2062', ' indul', 'sto', ' imp', 'Diver', 'hak', 'brow', ' commis']\n",
      "Value vec: Layer 1, index 8252\n",
      "[' AssemblyCulture', '}}{{', '\\u202f', 'kti', '[', 'ThroughAttribute', 'orkshire', '</i>', ' histor', '...']\n",
      "Value vec: Layer 7, index 7732\n",
      "['ValueStyle', 'BagLayout', 'StoryboardSegue', ' Мексичка', 'SBATCH', ' виправивши', '.\"]', '\")]\\r', 'FieldBuilder', '!\")\\r']\n",
      "Value vec: Layer 1, index 836\n",
      "[' Roskov', ' myſelf', 'HasAnnotation', 'MigrationBuilder', ' Seeder', 'WebServlet', ' nahilalakip', 'aarrggbb', 'Portale', ' pinulongan']\n",
      "Value vec: Layer 23, index 105\n",
      "['htë', 'httphttps', 'ImageIO', 'Jays', 'Portail', ' électron', 'igon', ' BitmapFactory', ' Мексичка', '談社']\n",
      "Value vec: Layer 17, index 8418\n",
      "[' idiot', ' idiots', ' Idiot', 'Idiot', 'idiot', ' stupid', ' moron', ' dumbass', ' morons', ' idiota']\n",
      "Value vec: Layer 14, index 5778\n",
      "['AccessorTable', ' crossorigin', ' betweenstory', 'UserScript', 'OGND', 'ThroughAttribute', 'RenderAtEndOf', 'tvguidetime', '\\ue315', ' non']\n",
      "Value vec: Layer 17, index 6239\n",
      "[' being', ' having', ' étant', ' needing', 'being', ' preferring', 'having', ' requiring', ' giving', ' fiind']\n",
      "Value vec: Layer 8, index 7765\n",
      "['#+#', '脚注の使い方', 'MENAFN', ' EconPapers', ' ModelExpression', 'ReusableCell', ' CURIAM', 'ništvo', 'SizeF', '\">—']\n",
      "Value vec: Layer 15, index 5723\n",
      "['++];', ' consultato', 'findpost', 'httphttps', 'ISupport', ')++;', ' UITableViewCell', 'ApiModelProperty', 'äsident', 'slidesToShow']\n",
      "Value vec: Layer 21, index 450\n",
      "['!!!', '!!!!', '!!!)', ' (', '!!', '!!!!!', '!!!!!!', '!!)', ' nakalista', '!!!!!!!']\n",
      "Value vec: Layer 25, index 3700\n",
      "['Datuak', ' onPostExecute', 'pyplot', ' QtCore', 'yntaxException', 'कारी', 'Row', ' uLocal', '})()', '⎨']\n",
      "Value vec: Layer 25, index 3793\n",
      "['RegressionTest', ' Moth', 'ariot', ' intStringLen', ' ISTAT', 'localctx', 'pacs', ' виправивши', ' לו', ' ant']\n",
      "Value vec: Layer 17, index 1761\n",
      "[' Houſe', ' houſe', ' Eſ', ' SBP', ' TSM', ' Theſe', ' ſtre', ' ſtate', ' Muses', ' greateſt']\n",
      "Value vec: Layer 15, index 194\n",
      "[' delays', ' delay', ' Delays', ' delaying', ' Delay', ' prolong', ' delayed', ' interminable', ' prolonged', ' DELAY']\n",
      "Value vec: Layer 19, index 6540\n",
      "[' himself', ' themselves', ' herself', 'themselves', ' itself', ' ourselves', 'himself', ' yourself', 'itself', ' Himself']\n",
      "Value vec: Layer 23, index 6406\n",
      "['WebElementEntity', ' propOrder', 'BufferException', ' gynhyrchwyd', 'expandindo', ' Reuter', 'helial', ' unknownFields', ' Roskov', ' betweenstory']\n",
      "Value vec: Layer 25, index 3875\n",
      "['rrggbb', 'InvalidProtocol', 'ILayout', 'BufferException', ' članak', 'onomía', 'AnimationsModule', '\">)</', 'initComponents', 'PRWEB']\n",
      "Value vec: Layer 2, index 4657\n",
      "['tlement', ' Dodson', 'enerated', 'érêt', 'elfare', ' تضيفلها', 'ocell', '<![', ' AppDelegate', 'xicity']\n",
      "Value vec: Layer 9, index 7915\n",
      "['AndEndTag', 'IUrlHelper', 'צלחה', ' Réponses', 'toare', ' OFDb', ' CreateTagHelper', '=?,', 'ătoare', ' gärna']\n",
      "Value vec: Layer 14, index 3133\n",
      "[' насељу', ' vPvB', ' LCCN', 'зулта', ' pú', 'TProtocol', 'SuspendLayout', ' indisponible', ' jij', 'FOOTNOTES']\n",
      "Value vec: Layer 25, index 1735\n",
      "[' calendriers', ' unknownFields', '\\ufeff/*', ' مشين', ' mxArray', '\\uf0f1', ' henvisninger', 'ệp', 'DOCTYPE', ' فريبيس']\n",
      "Value vec: Layer 2, index 8491\n",
      "['AutoScaleMode', 'ướp', 'ModelBuilder', 'oredCriteria', 'yntaxException', 'ctuations', 'ANSION', ' насељу', ' sereia', 'atka']\n"
     ]
    }
   ],
   "source": [
    "for (layer_idx, vec_idx) in top_neuron_tuples:\n",
    "    print(f\"Value vec: Layer {layer_idx}, index {vec_idx}\")\n",
    "    print(unembed_to_text(-1*value_vectors[layer_idx*2304*4+vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22a9138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "### Cossim between probe and toxic embedding\n",
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/gemma2_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fa6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "toxic_embed = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/gemma2_toxic_embed.pt\")\n",
    "print(toxic_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb4992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between toxic_embed and toxic_probe: 0.1319\n"
     ]
    }
   ],
   "source": [
    "toxic_embed_norm = F.normalize(toxic_embed, dim=0)\n",
    "toxic_probe_norm = F.normalize(toxic_probe, dim=0)\n",
    "\n",
    "cossim = torch.dot(toxic_embed_norm, toxic_probe_norm)\n",
    "\n",
    "print(f\"Cosine similarity between toxic_embed and toxic_probe: {cossim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7a227",
   "metadata": {},
   "source": [
    "#### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140848f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/mistral_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d98b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddec0e8934440089af0c3ab60848b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b6ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda670d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9714355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Value vec -> Layer 14, index 14693\n",
      "['shit', 'Fuck', 'shit', 'fuck', 'Block', 'piss', 'fucking', 'Diff', 'bitch', 'fucked']\n",
      "Rank 2: Value vec -> Layer 23, index 8307\n",
      "['proof', '-', 'book', 'bag', 'tag', 'cl', 'top', 'tale', 'p', 'ro']\n",
      "Rank 3: Value vec -> Layer 4, index 6683\n",
      "['uts', 'ylv', 'aly', 'caused', 'Gal', 'Wat', 'lymp', 'лей', 'cola', 'ness']\n",
      "Rank 4: Value vec -> Layer 25, index 2781\n",
      "['an', 'annotation', 'annot', '一个', 'a', 'expansion', 'ented', 'einer', 'owa', 'annotations']\n",
      "Rank 5: Value vec -> Layer 15, index 4854\n",
      "['caused', 'prevention', 'rez', 'committed', 'Castle', 'resc', 'Og', 'uous', 'affecting', 'mars']\n",
      "Rank 6: Value vec -> Layer 22, index 4327\n",
      "['Ü', 'aho', 'ví', 'olly', 'ège', 'umber', 'tres', 'Credit', 'MAGE', 'чё']\n",
      "Rank 7: Value vec -> Layer 13, index 7620\n",
      "['bru', 'stag', 'efore', 'xp', 'uru', 'rá', 'ʊ', 'charging', 'Cpp', 'owo']\n",
      "Rank 8: Value vec -> Layer 5, index 11009\n",
      "['export', 'rag', 'sek', 'deg', 'asa', 'cluster', 'clusters', 'cluster', 'industry', 'industries']\n",
      "Rank 9: Value vec -> Layer 25, index 6348\n",
      "['Fre', 'gener', 'stru', 'Ide', 'ép', 'ide', 'ugh', 'structural', 'TEMP', 'otherwise']\n",
      "Rank 10: Value vec -> Layer 0, index 1566\n",
      "['RST', 'iglia', 'anche', 'geme', 'ilities', 'illance', 'ュ', 'adel', 'osen', 'kte']\n",
      "Rank 11: Value vec -> Layer 5, index 11786\n",
      "['alone', 'oxy', 'GTH', 'left', 'dit', 'alone', 'isol', 'analyt', 'abandoned', 'carrying']\n",
      "Rank 12: Value vec -> Layer 13, index 9751\n",
      "['favourite', 'anca', 'iero', 'aglia', 'behaviour', 'diary', 'schen', 'riend', 'nit', 'ú']\n",
      "Rank 13: Value vec -> Layer 22, index 10939\n",
      "['alen', 'ogo', 'ós', 'eman', 'undial', 'ﬂ', 'rä', 'elles', 'ki', 'Caption']\n",
      "Rank 14: Value vec -> Layer 1, index 2741\n",
      "['--(', 'ride', 'istribute', 'eln', 'ķ', 'alias', '₦', '铺', 'conversion', '�']\n",
      "Rank 15: Value vec -> Layer 9, index 15077\n",
      "['dirty', 'Brun', 'Aur', 'oen', 'corre', 'eerst', 'correlation', 'cur', 'dirty', 'uren']\n",
      "Rank 16: Value vec -> Layer 14, index 8200\n",
      "['crap', 'nonsense', 'stupid', 'damn', 'ridiculous', 'damned', 'bullshit', 'shit', 'crazy', 'idi']\n",
      "Rank 17: Value vec -> Layer 8, index 15791\n",
      "['aya', 'sor', '(!', 'disc', 'cip', 'лта', \"('\", 'provoc', 'distinct', '(“']\n",
      "Rank 18: Value vec -> Layer 19, index 3645\n",
      "['lust', 'quire', 'sbur', 'Ukr', 'UA', 'republic', 'spread', 'wie', 'hö', 'dem']\n",
      "Rank 19: Value vec -> Layer 1, index 2793\n",
      "['flame', 'fine', 'ster', 'oot', 'achuset', 'synchronized', 'administrative', 'irt', 'neutral', 'flames']\n",
      "Rank 20: Value vec -> Layer 24, index 2385\n",
      "['ami', 'ymbol', 'anton', 'amy', 'aul', 'SA', 'bst', 'idel', '/******/', 'Sa']\n",
      "Rank 21: Value vec -> Layer 19, index 10393\n",
      "['ijo', 'Jorge', 'export', 'ialize', 'isl', 'iku', 'CAST', 'ropol', 'EXPORT', 'oge']\n",
      "Rank 22: Value vec -> Layer 0, index 13998\n",
      "['ons', 'sne', 'HI', 'ilon', 'ads', 'oin', 'hundreds', '#!', 'Address', 'hi']\n",
      "Rank 23: Value vec -> Layer 17, index 13248\n",
      "['ulus', 'sig', 'imer', 'drift', 'eli', 'Kre', 'Ci', 'elect', 'LO', 'Kurt']\n",
      "Rank 24: Value vec -> Layer 0, index 15567\n",
      "['◄', '/******/', 'ittel', 'reve', 'jours', 'IsNull', '?_', 'ölker', 'ześ', 'dule']\n",
      "Rank 25: Value vec -> Layer 17, index 14032\n",
      "['hell', 'damn', 'damned', 'fuck', 'Hell', 'crap', 'shit', 'hell', 'dam', 'Dam']\n",
      "Rank 26: Value vec -> Layer 12, index 9665\n",
      "['passes', 'interviews', 'proced', 'pow', 'oco', 'pert', 'ca', 'aily', 'interviewed', 'inen']\n",
      "Rank 27: Value vec -> Layer 20, index 1890\n",
      "['ächst', 'ља', 'icz', 'cken', 'avan', 'oggle', 'wig', 'gro', '[', 'gage']\n",
      "Rank 28: Value vec -> Layer 19, index 11588\n",
      "['{}', 'xspace', '{},', '\\\\', '{})', '\\\\/', 'iante', 'subs', '{}', '~']\n",
      "Rank 29: Value vec -> Layer 27, index 3488\n",
      "['oom', 'CE', 'ei', 'och', 'ATCH', 'doc', 'oz', 'rais', 'Bir', 'ъ']\n",
      "Rank 30: Value vec -> Layer 14, index 6787\n",
      "['ntil', 'azar', 'iny', 'oki', 'een', 'Crown', 'selected', 'Jul', 'Am', 'IX']\n",
      "Rank 31: Value vec -> Layer 5, index 9961\n",
      "['oshi', 'bid', 'eme', 'cov', 'entrepre', 'sanct', 'bind', 'vod', 'Municipal', 'ono']\n",
      "Rank 32: Value vec -> Layer 9, index 2442\n",
      "['ť', 'fony', 'bis', 'Cohen', 'Autow', 'świ', 'achuset', 'ąz', 'voce', 'ry']\n",
      "Rank 33: Value vec -> Layer 4, index 10737\n",
      "['aki', 'Fuck', 'fuck', 'disc', 'stru', 'Dol', 'hur', 'pic', 'ys', 'Di']\n",
      "Rank 34: Value vec -> Layer 15, index 4272\n",
      "['Dy', 'lish', 'env', 'stairs', 'hem', 'pract', 'polit', 'cred', 'stag', 'lov']\n",
      "Rank 35: Value vec -> Layer 7, index 6536\n",
      "['elles', 'ган', 'Ort', 'rus', 'icus', 'Bis', 'eller', 'dom', 'erts', 'ormal']\n",
      "Rank 36: Value vec -> Layer 12, index 8139\n",
      "['fucked', 'fuck', 'shit', 'bitch', 'sex', 'sexual', 'fucking', 'rape', 'piss', 'suck']\n",
      "Rank 37: Value vec -> Layer 4, index 7551\n",
      "['widet', 'oz', 'upp', 'Tat', 'anger', 'iac', 'anz', 'orage', 'porte', 'Wol']\n",
      "Rank 38: Value vec -> Layer 23, index 11609\n",
      "['nasty', 'nast', 'sol', 'rezent', 'sol', 'erra', 'ket', 'poon', 'itaire', 'UC']\n",
      "Rank 39: Value vec -> Layer 26, index 905\n",
      "['bur', 'bur', 'Bur', 'inwon', 'INGS', 'apan', 'Hur', 'burden', 'figure', 'amb']\n",
      "Rank 40: Value vec -> Layer 17, index 12272\n",
      "['packing', 'packed', 'amount', 'fit', 'pack', 'packed', 'loads', 'fitting', 'amount', 'tons']\n",
      "Rank 41: Value vec -> Layer 20, index 16224\n",
      "['up', 'up', 'ups', 'Up', 'Up', 'UP', 'uper', 'UP', 'upt', 'upid']\n",
      "Rank 42: Value vec -> Layer 19, index 12634\n",
      "['pless', 'burgh', 'Atl', 'Sister', 'Martin', 'Ni', 'bare', 'Bez', 'Adams', 'Mike']\n",
      "Rank 43: Value vec -> Layer 18, index 3604\n",
      "['dom', 'DOM', 'dom', 'oa', 'Jar', 'bery', 'ery', 'Chart', 'ston', 'gang']\n",
      "Rank 44: Value vec -> Layer 26, index 9184\n",
      "['/******/', 'avax', 'agma', 'orph', 'typen', 'ntax', 'akespe', 'odox', 'ště', 'tocol']\n",
      "Rank 45: Value vec -> Layer 5, index 2359\n",
      "['ând', 'ync', 'ât', 'agi', 'rå', 'eston', 'usher', 'camb', 'ёл', 'aje']\n",
      "Rank 46: Value vec -> Layer 26, index 3186\n",
      "['ich', 'iel', 'ib', 'cio', 'Tib', 'el', 'ons', 'robe', 'organic', 'min']\n",
      "Rank 47: Value vec -> Layer 20, index 2297\n",
      "['iju', 'ti', 'acht', 'è', 'anj', 'ęd', 'egen', 'maz', 'ijd', 'wig']\n",
      "Rank 48: Value vec -> Layer 2, index 5396\n",
      "['idos', 'RC', 'endor', 'fa', 'idth', 'skill', 'FA', 'ica', 'ur', 'comprehens']\n",
      "Rank 49: Value vec -> Layer 20, index 14404\n",
      "['uits', 'cob', 'uld', '/******/', 'bul', 'ych', 'zte', 'abul', 'icus', 'hierarchy']\n",
      "Rank 50: Value vec -> Layer 15, index 6760\n",
      "['VL', 'intellig', 'rates', 'eras', 'Cow', 'chos', '◄', 'ODO', 'veloc', 'ch']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 50\n",
    "norm = model.model.norm  \n",
    "\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k, largest=False).indices\n",
    "top_scores = dot_prods[top_value_vecs]\n",
    "\n",
    "for rank, vec_idx in enumerate(top_value_vecs):\n",
    "    layer = vec_idx.item() // (4096*4)\n",
    "    index = vec_idx.item() % (4096*4)\n",
    "    \n",
    "    print(f\"Rank {rank + 1}: Value vec -> Layer {layer}, index {index}\")\n",
    "    print(unembed_to_text(-1*value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571470e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "### Cossim between probe and toxic embedding\n",
    "toxic_embed = torch.load(\"/data/kebl6672/dpo-toxic-general/ignore/mistral_toxic_embed.pt\")\n",
    "print(toxic_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9369a65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between toxic_embed and toxic_probe: 0.1095\n"
     ]
    }
   ],
   "source": [
    "toxic_embed_norm = F.normalize(toxic_embed, dim=0)\n",
    "toxic_probe_norm = F.normalize(toxic_probe, dim=0)\n",
    "\n",
    "cossim = torch.dot(toxic_embed_norm, toxic_probe_norm)\n",
    "\n",
    "print(f\"Cosine similarity between toxic_embed and toxic_probe: {cossim.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
